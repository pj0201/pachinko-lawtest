#!/usr/bin/env python3
"""
é‡è¤‡å•é¡Œæ’é™¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - Union-Find ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
å®Œå…¨é‡è¤‡ï¼ˆ99%ä»¥ä¸Šï¼‰ã‚’æ¤œå‡ºãƒ»æ’é™¤
"""

import json
from pathlib import Path
from difflib import SequenceMatcher
from collections import defaultdict

INPUT_FILE = Path("/home/planj/patshinko-exam-app/data/CORRECT_1491_PROBLEMS_WITH_LEGAL_REFS.json")
OUTPUT_FILE = Path("/home/planj/patshinko-exam-app/data/DEDUPED_BASE.json")

def text_similarity(text1, text2):
    """ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
    return SequenceMatcher(None, text1, text2).ratio()

def find_duplicate_groups(problems, threshold=0.99):
    """é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ¤œå‡º"""
    duplicate_map = defaultdict(set)
    total = len(problems)

    print("ğŸ” é‡è¤‡æ¤œå‡ºä¸­...")

    for i, p1 in enumerate(problems):
        if i % 200 == 0:
            print(f"  é€²æ—: {i}/{total}")

        text1 = p1.get('problem_text', '')
        id1 = p1.get('problem_id')

        for p2 in problems[i+1:]:
            text2 = p2.get('problem_text', '')
            id2 = p2.get('problem_id')

            similarity = text_similarity(text1, text2)

            if similarity >= threshold:
                duplicate_map[id1].add(id2)
                duplicate_map[id2].add(id1)

    # BFSã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    visited = set()
    groups = []

    for pid in duplicate_map:
        if pid in visited:
            continue

        group = set()
        queue = [pid]

        while queue:
            current = queue.pop(0)
            if current in visited:
                continue

            visited.add(current)
            group.add(current)

            for neighbor in duplicate_map[current]:
                if neighbor not in visited:
                    queue.append(neighbor)

        if group:
            groups.append(group)

    return groups

def remove_duplicates(input_file, output_file):
    """é‡è¤‡å•é¡Œã‚’æ’é™¤"""
    print("=" * 80)
    print("é‡è¤‡å•é¡Œæ’é™¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 80)

    # ãƒ­ãƒ¼ãƒ‰
    print(f"\nğŸ“‚ {input_file} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    problems = data.get('problems', [])
    original_count = len(problems)
    print(f"  å…ƒã®å•é¡Œæ•°: {original_count}å•")

    # é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ¤œå‡º
    print("\nğŸ” é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ¤œå‡ºä¸­...")
    groups = find_duplicate_groups(problems, threshold=0.99)
    print(f"  é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(groups)}")

    # çµ±è¨ˆæƒ…å ±
    total_duplicates = sum(len(g) - 1 for g in groups)
    duplicate_pairs = sum(len(g) // 2 for g in groups)
    print(f"  é‡è¤‡ãƒšã‚¢æ•°: ç´„{duplicate_pairs}ãƒšã‚¢")
    print(f"  é‡è¤‡ã«é–¢ä¸ã™ã‚‹å•é¡Œ: ç´„{total_duplicates * 2}å•")

    # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’ä¿æŒã™ã‚‹ãƒãƒƒãƒ—ã‚’ä½œæˆ
    problem_by_id = {p['problem_id']: p for p in problems}

    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰1ã¤ãšã¤é¸æŠ
    print("\nğŸ”€ å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æœ€é©ãªå•é¡Œã‚’é¸æŠä¸­...")
    unique_problems = []
    removed_count = 0

    # ã‚°ãƒ«ãƒ¼ãƒ—ã«å±ã•ãªã„å•é¡Œã‚’è¿½åŠ 
    all_duplicate_ids = set()
    for group in groups:
        all_duplicate_ids.update(group)

    for p in problems:
        if p.get('problem_id') not in all_duplicate_ids:
            unique_problems.append(p)

    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰1ã¤é¸æŠ
    for group_idx, group in enumerate(groups):
        # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§æœ€å°ã®IDã‚’æŒã¤å•é¡Œã‚’é¸æŠï¼ˆå¤ã„å•é¡Œã‚’å„ªå…ˆï¼‰
        best_id = min(group)
        best_problem = problem_by_id[best_id]
        unique_problems.append(best_problem)
        removed_count += len(group) - 1

        if (group_idx + 1) % 100 == 0:
            print(f"  å‡¦ç†æ¸ˆã¿ã‚°ãƒ«ãƒ¼ãƒ—: {group_idx + 1}/{len(groups)}")

    # çµæœ
    final_count = len(unique_problems)
    print(f"\nâœ… é‡è¤‡æ’é™¤å®Œäº†")
    print(f"  å…ƒã®å•é¡Œæ•°: {original_count}å•")
    print(f"  å‰Šé™¤ã•ã‚ŒãŸå•é¡Œæ•°: {removed_count}å•")
    print(f"  é‡è¤‡æ’é™¤å¾Œ: {final_count}å•")
    print(f"  å‰Šæ¸›ç‡: {(removed_count / original_count * 100):.1f}%")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    data['problems'] = unique_problems
    data['metadata']['total_problems'] = final_count
    data['metadata']['version'] = "DEDUPED_1.0"
    data['metadata']['deduplication'] = {
        'original_count': original_count,
        'removed_count': removed_count,
        'final_count': final_count,
        'duplicate_groups': len(groups)
    }

    # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã‚’è¨ˆç®—
    from collections import Counter
    category_counts = Counter(p['category'] for p in unique_problems)
    data['metadata']['statistics']['category_distribution'] = dict(category_counts)

    # ä¿å­˜
    print(f"\nğŸ’¾ {output_file} ã«ä¿å­˜ä¸­...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å®Œäº†ï¼\n")

    # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã®è¡¨ç¤º
    print("ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å•é¡Œæ•°:")
    for cat in sorted(category_counts.keys()):
        count = category_counts[cat]
        pct = (count / final_count) * 100
        print(f"  {cat}: {count}å• ({pct:.1f}%)")

    print("=" * 80)

if __name__ == '__main__':
    remove_duplicates(INPUT_FILE, OUTPUT_FILE)
