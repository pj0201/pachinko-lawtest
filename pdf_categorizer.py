#!/usr/bin/env python3
"""
ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã‚¨ãƒ³ã‚¸ãƒ³
OCRçµæœã‚’ä¸»ä»»è€…è©¦é¨“ã®æ¨™æº–ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†é¡
"""

import json
from collections import defaultdict
from pathlib import Path

# ==================== ã‚«ãƒ†ã‚´ãƒªãƒ¼å®šç¾© ====================

CATEGORIES = {
    'æ³•å¾‹çŸ¥è­˜': {
        'keywords': [
            'é¢¨ä¿—å–¶æ¥­æ³•', 'æ³•å¾‹', 'æ³•ä»¤', 'æ³•è¦', 'è¦å®š', 'è¦å‰‡',
            'é¢¨ä¿—å–¶æ¥­', 'å–¶æ¥­ç¦æ­¢', 'ç¦æ­¢', 'è¨±å¯', 'ç”³è«‹', 'æœŸé™',
            'ç½°å‰‡', 'åˆ‘ç½°', 'é•å', 'ç½°é‡‘', 'æ‡²å½¹', 'è¦ä»¶'
        ],
        'weight': 1.0
    },
    'å–¶æ¥­ç®¡ç†': {
        'keywords': [
            'å–¶æ¥­', 'ãƒ›ãƒ¼ãƒ«', 'åº—èˆ—', 'æ¥­å‹™', 'ç®¡ç†', 'é‹å–¶',
            'å¾“æ¥­å“¡', 'ã‚¹ã‚¿ãƒƒãƒ•', 'ã‚¢ãƒ«ãƒã‚¤ãƒˆ', 'ã‚·ãƒ•ãƒˆ',
            'å¸³ç°¿', 'è¨˜éŒ²', 'å ±å‘Š', 'ç”³å‘Š', 'ç”³è«‹æ›¸', 'æ›´æ–°',
            'è¨±å¯ç”³è«‹', 'æ›´æ–°ç”³è«‹', 'ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹', 'ãƒã‚§ãƒƒã‚¯',
            'å–¶æ¥­æ™‚é–“', 'å–¶æ¥­åŒºåŸŸ', 'å–¶æ¥­æ—¥', 'å–¶æ¥­æ‰€', 'çœ‹æ¿'
        ],
        'weight': 0.9
    },
    'æ©Ÿæ¢°çŸ¥è­˜': {
        'keywords': [
            'éŠæŠ€æ©Ÿ', 'ãƒ‘ãƒãƒ³ã‚³', 'ãƒ‘ãƒã‚¹ãƒ­', 'ã‚¹ãƒ­ãƒƒãƒˆ', 'æ©Ÿæ¢°',
            'æ©Ÿå™¨', 'ãƒ¡ãƒ€ãƒ«', 'ãƒœãƒ¼ãƒ«', 'ç‰', 'å°', 'å°æ•°',
            'è¨­ç½®', 'æ’¤å»', 'äº¤æ›', 'ä¿®ç†', 'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹',
            'å‹å¼', 'èªå®š', 'èªå¯', 'åŸºæº–', 'ä»•æ§˜', 'æ€§èƒ½',
            'éƒ¨å“', 'å‹•ä½œ', 'æ©Ÿèƒ½', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒ ', 'ROM', 'æ¤œå®š'
        ],
        'weight': 1.0
    },
    'å–¶æ¥­é–‹å§‹': {
        'keywords': [
            'é–‹å§‹', 'é–‹æ¥­', 'æ–°è¦', 'æ–°è¨­', 'é–‹è¨­', 'èµ·æ¥­',
            'è³‡æ ¼', 'ç”³è«‹', 'ãƒã‚§ãƒƒã‚¯', 'æ¡ä»¶', 'è¦ä»¶', 'åŸºæº–'
        ],
        'weight': 0.7
    }
}

# ==================== ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç† ====================

def preprocess_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–"""
    # æ”¹è¡Œã‚’ç©ºç™½ã«ç½®æ›
    text = text.replace('\n', ' ').replace('\r', ' ')
    # è¤‡æ•°ã®ç©ºç™½ã‚’1ã¤ã«
    text = ' '.join(text.split())
    # å°æ–‡å­—ã«çµ±ä¸€ï¼ˆãŸã ã—æ—¥æœ¬èªã¯ä¿æŒï¼‰
    return text

# ==================== ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚° ====================

def calculate_keyword_score(text, category_keywords):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    text_lower = text.lower()
    score = 0
    matches = []

    for keyword in category_keywords:
        keyword_lower = keyword.lower()
        count = text.count(keyword_lower)

        if count > 0:
            # å‡ºç¾å›æ•° Ã— ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é•· ã§ã‚¹ã‚³ã‚¢åŒ–
            # ï¼ˆé•·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ–¹ãŒé‡è¦ï¼‰
            keyword_score = count * (len(keyword) / 5)
            score += keyword_score
            matches.append({
                'keyword': keyword,
                'count': count,
                'score': keyword_score
            })

    return score, matches

# ==================== TF-IDF é¢¨ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° ====================

def calculate_tfidf_score(text, category_keywords):
    """TF-IDFé¢¨ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    words = text.split()
    word_count = len(words)

    if word_count == 0:
        return 0, []

    score = 0
    term_frequencies = {}

    # Term Frequency (TF) è¨ˆç®—
    for word in words:
        word_lower = word.lower()
        term_frequencies[word_lower] = term_frequencies.get(word_lower, 0) + 1

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã®ç…§åˆ
    matches = []
    for keyword in category_keywords:
        keyword_lower = keyword.lower()

        # å®Œå…¨ä¸€è‡´
        if keyword_lower in term_frequencies:
            tf = term_frequencies[keyword_lower] / word_count
            idf = 1.0 / len(category_keywords)  # ç°¡æ˜“IDF
            tfidf = tf * idf
            score += tfidf
            matches.append({
                'keyword': keyword,
                'tf': tf,
                'tfidf': tfidf
            })

        # éƒ¨åˆ†ä¸€è‡´ï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
        elif len(keyword_lower) >= 2:
            for word, count in term_frequencies.items():
                if keyword_lower in word:
                    tf = count / word_count
                    partial_score = tf * 0.5  # éƒ¨åˆ†ä¸€è‡´ã¯50%ã®ã‚¹ã‚³ã‚¢
                    score += partial_score
                    if not any(m['keyword'] == keyword for m in matches):
                        matches.append({
                            'keyword': keyword,
                            'partial_match': word,
                            'score': partial_score
                        })

    return score, matches

# ==================== ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¤å®šã‚¨ãƒ³ã‚¸ãƒ³ ====================

class CategoryClassifier:
    """å•é¡Œæ–‡ã‚’ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†é¡"""

    def __init__(self):
        self.categories = CATEGORIES

    def classify(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†é¡"""
        if not text or len(text.strip()) < 5:
            return 'ãã®ä»–', 0.0, {}

        # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
        clean_text = preprocess_text(text)

        # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        scores = {}
        details = {}

        for category, config in self.categories.items():
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢
            kw_score, kw_matches = calculate_keyword_score(
                clean_text,
                config['keywords']
            )

            # TF-IDF ã‚¹ã‚³ã‚¢
            tfidf_score, tfidf_matches = calculate_tfidf_score(
                clean_text,
                config['keywords']
            )

            # æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ã‘ï¼‰
            final_score = (kw_score * 0.6 + tfidf_score * 0.4) * config['weight']

            scores[category] = final_score
            details[category] = {
                'keyword_score': kw_score,
                'tfidf_score': tfidf_score,
                'final_score': final_score,
                'keyword_matches': kw_matches[:5],  # Top 5
                'tfidf_matches': tfidf_matches[:3]  # Top 3
            }

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’é¸æŠ
        best_category = max(scores, key=scores.get)
        best_score = scores[best_category]

        # ã‚¹ã‚³ã‚¢ãŒå…¨ã¦ä½ã„å ´åˆã¯ã€Œãã®ä»–ã€ã«åˆ†é¡
        if best_score < 0.1:
            best_category = 'ãã®ä»–'
            best_score = 0.0

        return best_category, best_score, details

    def classify_questions(self, questions):
        """è¤‡æ•°ã®å•é¡Œã‚’ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†é¡"""
        classified = []

        for q in questions:
            category, score, details = self.classify(q.get('text', ''))

            classified.append({
                **q,
                'category': category,
                'category_confidence': round(score, 3),
                'classification_details': details
            })

        return classified

# ==================== çµ±è¨ˆæƒ…å ±ç”Ÿæˆ ====================

def generate_statistics(classified_questions):
    """ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥çµ±è¨ˆã‚’ç”Ÿæˆ"""
    stats = defaultdict(lambda: {'count': 0, 'avg_confidence': 0})

    for q in classified_questions:
        category = q['category']
        stats[category]['count'] += 1
        stats[category]['avg_confidence'] += q['category_confidence']

    # å¹³å‡ä¿¡é ¼åº¦ã‚’è¨ˆç®—
    for category in stats:
        count = stats[category]['count']
        if count > 0:
            stats[category]['avg_confidence'] /= count
            stats[category]['avg_confidence'] = round(stats[category]['avg_confidence'], 3)

    return dict(stats)

# ==================== ãƒ¡ã‚¤ãƒ³å‡¦ç† ====================

def process_ocr_results(ocr_results_file, output_file):
    """OCRçµæœã‚’åˆ†é¡ãƒ»çµ±è¨ˆåŒ–"""
    print("=" * 70)
    print("ğŸ” ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 70)

    # OCRçµæœã‚’èª­ã¿è¾¼ã¿
    with open(ocr_results_file, 'r', encoding='utf-8') as f:
        ocr_results = json.load(f)

    print(f"\nğŸ“Š å…¥åŠ›: {len(ocr_results)}ãƒšãƒ¼ã‚¸ã®OCRçµæœ")

    # å•é¡Œã‚’æŠ½å‡ºï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚·ãƒ³ãƒ—ãƒ«ã«åˆ†å‰²ï¼‰
    from ocrToQuestions import extractQuestionsFromOCR
    questions = extractQuestionsFromOCR(ocr_results)

    print(f"âœ… æŠ½å‡ºå•é¡Œ: {len(questions)}å•")

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡
    classifier = CategoryClassifier()
    classified = classifier.classify_questions(questions)

    # çµ±è¨ˆæƒ…å ±ç”Ÿæˆ
    stats = generate_statistics(classified)

    print(f"\nğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥åˆ†å¸ƒ:")
    for category, info in stats.items():
        print(f"   {category}: {info['count']}å• (ä¿¡é ¼åº¦: {info['avg_confidence']:.1%})")

    # çµæœã‚’ä¿å­˜
    output_data = {
        'timestamp': str(__import__('datetime').datetime.now().isoformat()),
        'total_questions': len(classified),
        'statistics': stats,
        'questions': classified
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… åˆ†é¡çµæœä¿å­˜: {output_file}")
    print("=" * 70)

    return output_data

# ==================== ãƒ†ã‚¹ãƒˆç”¨ ====================

if __name__ == '__main__':
    import sys

    ocr_file = sys.argv[1] if len(sys.argv) > 1 else '/home/planj/patshinko-exam-app/data/ocr_results.json'
    output = sys.argv[2] if len(sys.argv) > 2 else '/home/planj/patshinko-exam-app/data/classified_questions.json'

    if Path(ocr_file).exists():
        process_ocr_results(ocr_file, output)
    else:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ocr_file}")
