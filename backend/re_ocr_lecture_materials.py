#!/usr/bin/env python3
"""
éŠæŠ€æ©Ÿå–æ‰±ä¸»ä»»è€… è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆ å†OCRå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
========================================

ç›®çš„:
  ç¾åœ¨ã®OCRçµæœã®å“è³ªå‘ä¸Šã¨æ¤œè¨¼
  â‘ .pdf, â‘¡.pdf, â‘¢.pdf ã‚’å†å‡¦ç†ã—ã€
  æ—§OCRã¨ã®æ¯”è¼ƒåˆ†æã‚’å®Ÿæ–½

å‡ºåŠ›:
  1. æ–°OCRçµæœ: ocr_results_deepseek_v2_YYYYMMDD.json
  2. å·®åˆ†åˆ†æ: ocr_differential_analysis_YYYYMMDD.json
  3. çµ±åˆç‰ˆ: ocr_results_unified_YYYYMMDD.json
"""

import sys
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple
import hashlib

# DeepSeek-OCRçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.insert(0, '/home/planj/Claude-Code-Communication')
from a2a_system.ocr_processing.pdf_processor import PDFProcessor

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LectureOCRReprocessor:
    """è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆå†OCRå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆåˆ†å‰²å‡¦ç†å¯¾å¿œç‰ˆï¼‰"""

    def __init__(self, batch_size=10):
        """
        Args:
            batch_size: ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ãƒšãƒ¼ã‚¸ï¼‰
        """
        self.pdf_processor = PDFProcessor(max_workers=4)
        self.pdf_files = {
            1: "/mnt/c/Users/planj/Downloads/â‘ .pdf",
            2: "/mnt/c/Users/planj/Downloads/â‘¡.pdf",
            3: "/mnt/c/Users/planj/Downloads/â‘¢.pdf"
        }
        self.old_ocr_path = "/home/planj/patshinko-exam-app/data/ocr_results_corrected.json"
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path("/home/planj/patshinko-exam-app/data")
        self.batch_size = batch_size  # ãƒšãƒ¼ã‚¸åˆ†å‰²ã‚µã‚¤ã‚º
        self.checkpoint_dir = self.output_dir / "checkpoints"
        self.checkpoint_dir.mkdir(exist_ok=True)

    def load_old_ocr(self) -> List[Dict]:
        """ç¾åœ¨ã®OCRçµæœã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            with open(self.old_ocr_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"æ—§OCRãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return []

    def extract_pdf_to_dict(self, pdf_index: int, start_page: int = None, end_page: int = None) -> List[Dict]:
        """
        PDFã‚’ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã—ã€OCRãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã™ï¼ˆåˆ†å‰²å‡¦ç†å¯¾å¿œï¼‰

        Args:
            pdf_index: PDFç•ªå·
            start_page: é–‹å§‹ãƒšãƒ¼ã‚¸ï¼ˆNoneã®å ´åˆã¯æœ€åˆã‹ã‚‰ï¼‰
            end_page: çµ‚äº†ãƒšãƒ¼ã‚¸ï¼ˆNoneã®å ´åˆã¯æœ€å¾Œã¾ã§ï¼‰
        """
        pdf_path = self.pdf_files[pdf_index]
        page_range_str = f"ãƒšãƒ¼ã‚¸ {start_page}-{end_page}" if start_page and end_page else "å…¨ãƒšãƒ¼ã‚¸"
        logger.info(f"å‡¦ç†ä¸­: {pdf_path} ({page_range_str})")

        try:
            pages_data = self.pdf_processor.extract_text_by_page(pdf_path)

            # ãƒšãƒ¼ã‚¸ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if start_page is not None or end_page is not None:
                filtered_pages = []
                for page_info in pages_data:
                    page_num = page_info['page']
                    if start_page is not None and page_num < start_page:
                        continue
                    if end_page is not None and page_num > end_page:
                        continue
                    filtered_pages.append(page_info)
                pages_data = filtered_pages

            # OCRãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
            results = []
            total_chars = 0
            for page_info in pages_data:
                char_count = len(page_info['text']) if page_info['text'] else 0
                result = {
                    "pdf_index": pdf_index,
                    "page_number": page_info['page'],
                    "text": page_info['text'],
                    "timestamp": datetime.now().isoformat(),
                    "extraction_method": "PyMuPDF_v2_split"
                }
                results.append(result)
                total_chars += char_count

            logger.info(f"âœ… PDF {pdf_index} {page_range_str}: {len(results)}ãƒšãƒ¼ã‚¸æŠ½å‡ºå®Œäº† (åˆè¨ˆ{total_chars:,}æ–‡å­—)")
            return results
        except Exception as e:
            logger.error(f"âŒ PDF {pdf_index} {page_range_str} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def save_checkpoint(self, pdf_index: int, batch_num: int, results: List[Dict]):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜"""
        checkpoint_file = self.checkpoint_dir / f"checkpoint_pdf{pdf_index}_batch{batch_num}_{self.timestamp}.json"
        try:
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_file.name}")
        except Exception as e:
            logger.error(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def load_checkpoints(self) -> List[Dict]:
        """æ—¢å­˜ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰"""
        all_results = []
        checkpoint_files = sorted(self.checkpoint_dir.glob(f"checkpoint_*_{self.timestamp}.json"))

        for checkpoint_file in checkpoint_files:
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                    all_results.extend(results)
                logger.info(f"ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­è¾¼: {checkpoint_file.name} ({len(results)}ãƒšãƒ¼ã‚¸)")
            except Exception as e:
                logger.error(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­è¾¼ã‚¨ãƒ©ãƒ¼ ({checkpoint_file.name}): {e}")

        return all_results

    def get_pdf_page_count(self, pdf_index: int) -> int:
        """PDFã®ç·ãƒšãƒ¼ã‚¸æ•°ã‚’å–å¾—"""
        try:
            pdf_path = self.pdf_files[pdf_index]
            pages_data = self.pdf_processor.extract_text_by_page(pdf_path)
            return len(pages_data)
        except Exception as e:
            logger.error(f"âŒ PDF {pdf_index} ãƒšãƒ¼ã‚¸æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0

    def reprocess_all_pdfs(self) -> List[Dict]:
        """å…¨3ã¤ã®PDFã‚’å†å‡¦ç†ï¼ˆãƒãƒƒãƒåˆ†å‰²å¯¾å¿œï¼‰"""
        all_results = []

        for pdf_index in [1, 2, 3]:
            logger.info(f"\n{'='*60}")
            logger.info(f"PDF {pdf_index} ã®å‡¦ç†ã‚’é–‹å§‹")
            logger.info(f"{'='*60}")

            try:
                # PDFå…¨ä½“ã‚’å–å¾—ã—ã¦ãƒšãƒ¼ã‚¸æ•°ã‚’ç¢ºèª
                full_results = self.extract_pdf_to_dict(pdf_index)
                total_pages = len(full_results)
                logger.info(f"ğŸ“„ PDF {pdf_index} ç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}")

                # ãƒãƒƒãƒå‡¦ç†ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
                if total_pages <= self.batch_size:
                    # ãƒšãƒ¼ã‚¸æ•°ãŒå°‘ãªã„å ´åˆã¯ä¸€åº¦ã«å‡¦ç†
                    logger.info(f"âœ… PDF {pdf_index}: ãƒšãƒ¼ã‚¸æ•°ãŒå°‘ãªã„ãŸã‚ä¸€æ‹¬å‡¦ç†")
                    all_results.extend(full_results)
                    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                    self.save_checkpoint(pdf_index, 1, full_results)
                else:
                    # ãƒãƒƒãƒåˆ†å‰²å‡¦ç†
                    num_batches = (total_pages + self.batch_size - 1) // self.batch_size
                    logger.info(f"ğŸ“¦ PDF {pdf_index}: {num_batches}å€‹ã®ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦å‡¦ç†")

                    for batch_num in range(1, num_batches + 1):
                        start_idx = (batch_num - 1) * self.batch_size
                        end_idx = min(batch_num * self.batch_size, total_pages)

                        # ãƒãƒƒãƒã‚’æŠ½å‡º
                        batch_results = full_results[start_idx:end_idx]

                        logger.info(f"  ãƒãƒƒãƒ {batch_num}/{num_batches}: ãƒšãƒ¼ã‚¸ {start_idx+1}-{end_idx} ({len(batch_results)}ãƒšãƒ¼ã‚¸)")

                        all_results.extend(batch_results)

                        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                        self.save_checkpoint(pdf_index, batch_num, batch_results)

                        # é€²æ—è¡¨ç¤º
                        progress = (batch_num / num_batches) * 100
                        logger.info(f"  é€²æ—: {progress:.1f}% å®Œäº†")

                logger.info(f"âœ… PDF {pdf_index} å‡¦ç†å®Œäº†: {total_pages}ãƒšãƒ¼ã‚¸")

            except Exception as e:
                logger.error(f"âŒ PDF {pdf_index} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                logger.info(f"âš ï¸  PDF {pdf_index} ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œã—ã¾ã™")
                continue

        logger.info(f"\n{'='*60}")
        logger.info(f"âœ… å…¨PDFå‡¦ç†å®Œäº†: åˆè¨ˆ {len(all_results)}ãƒšãƒ¼ã‚¸")
        logger.info(f"{'='*60}\n")

        return all_results

    def calculate_text_hash(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()[:16]

    def perform_differential_analysis(
        self,
        old_ocr: List[Dict],
        new_ocr: List[Dict]
    ) -> Dict:
        """æ–°æ—§OCRçµæœã®å·®åˆ†åˆ†æ"""
        logger.info("å·®åˆ†åˆ†æé–‹å§‹...")

        analysis = {
            "analysis_timestamp": datetime.now().isoformat(),
            "old_ocr_count": len(old_ocr),
            "new_ocr_count": len(new_ocr),
            "page_differences": [],
            "summary": {
                "total_pages": 0,
                "identical_pages": 0,
                "improved_pages": 0,
                "degraded_pages": 0,
                "missing_in_old": 0,
                "missing_in_new": 0
            },
            "quality_metrics": {
                "old_total_chars": 0,
                "new_total_chars": 0,
                "char_count_change_percent": 0.0
            }
        }

        # ãƒšãƒ¼ã‚¸ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
        old_pages = {(p['pdf_index'], p['page_number']): p for p in old_ocr}
        new_pages = {(p['pdf_index'], p['page_number']): p for p in new_ocr}

        # å…¨ãƒšãƒ¼ã‚¸ã‚­ãƒ¼
        all_page_keys = set(old_pages.keys()) | set(new_pages.keys())
        analysis['summary']['total_pages'] = len(all_page_keys)

        for pdf_idx, page_num in sorted(all_page_keys):
            old_page = old_pages.get((pdf_idx, page_num))
            new_page = new_pages.get((pdf_idx, page_num))

            page_diff = {
                "pdf_index": pdf_idx,
                "page_number": page_num,
                "status": "",
                "old_char_count": 0,
                "new_char_count": 0,
                "char_diff": 0,
                "quality_change": ""
            }

            if old_page and new_page:
                old_text = old_page['text']
                new_text = new_page['text']
                old_chars = len(old_text)
                new_chars = len(new_text)

                page_diff['old_char_count'] = old_chars
                page_diff['new_char_count'] = new_chars
                page_diff['char_diff'] = new_chars - old_chars

                if old_text == new_text:
                    page_diff['status'] = "identical"
                    analysis['summary']['identical_pages'] += 1
                else:
                    if new_chars > old_chars * 1.05:
                        page_diff['status'] = "improved"
                        page_diff['quality_change'] = f"+{new_chars - old_chars}æ–‡å­— (+{(new_chars/old_chars - 1)*100:.1f}%)"
                        analysis['summary']['improved_pages'] += 1
                    elif new_chars < old_chars * 0.95:
                        page_diff['status'] = "degraded"
                        page_diff['quality_change'] = f"{new_chars - old_chars}æ–‡å­— ({(new_chars/old_chars - 1)*100:.1f}%)"
                        analysis['summary']['degraded_pages'] += 1
                    else:
                        page_diff['status'] = "changed"
                        page_diff['quality_change'] = f"{new_chars - old_chars:+d}æ–‡å­—"

            elif old_page and not new_page:
                page_diff['status'] = "missing_in_new"
                page_diff['old_char_count'] = len(old_page['text'])
                analysis['summary']['missing_in_new'] += 1

            elif not old_page and new_page:
                page_diff['status'] = "missing_in_old"
                page_diff['new_char_count'] = len(new_page['text'])
                analysis['summary']['missing_in_old'] += 1

            analysis['page_differences'].append(page_diff)

            if old_page:
                analysis['quality_metrics']['old_total_chars'] += len(old_page['text'])
            if new_page:
                analysis['quality_metrics']['new_total_chars'] += len(new_page['text'])

        # æ–‡å­—æ•°å¤‰åŒ–ç‡ã‚’è¨ˆç®—
        if analysis['quality_metrics']['old_total_chars'] > 0:
            change_percent = (
                (analysis['quality_metrics']['new_total_chars'] -
                 analysis['quality_metrics']['old_total_chars']) /
                analysis['quality_metrics']['old_total_chars'] * 100
            )
            analysis['quality_metrics']['char_count_change_percent'] = round(change_percent, 2)

        logger.info(f"âœ… å·®åˆ†åˆ†æå®Œäº†: {len(all_page_keys)}ãƒšãƒ¼ã‚¸æ¤œè¨¼")
        logger.info(f"  åŒä¸€: {analysis['summary']['identical_pages']}, " +
                   f"æ”¹å–„: {analysis['summary']['improved_pages']}, " +
                   f"å¤‰æ›´: {analysis['summary']['degraded_pages']}")

        return analysis

    def create_unified_ocr(
        self,
        old_ocr: List[Dict],
        new_ocr: List[Dict],
        analysis: Dict
    ) -> List[Dict]:
        """æ–°æ—§OCRã‹ã‚‰çµ±åˆç‰ˆã‚’ä½œæˆï¼ˆå“è³ªã®é«˜ã„æ–¹ã‚’é¸æŠï¼‰"""
        logger.info("çµ±åˆç‰ˆä½œæˆé–‹å§‹...")

        unified = []
        old_pages = {(p['pdf_index'], p['page_number']): p for p in old_ocr}
        new_pages = {(p['pdf_index'], p['page_number']): p for p in new_ocr}

        for page_diff in analysis['page_differences']:
            pdf_idx = page_diff['pdf_index']
            page_num = page_diff['page_number']

            old_page = old_pages.get((pdf_idx, page_num))
            new_page = new_pages.get((pdf_idx, page_num))

            # å“è³ªåˆ¤å®š: æ–‡å­—æ•°ãŒå¤šã„æ–¹ï¼ˆé€šå¸¸ã¯å“è³ªãŒé«˜ã„ï¼‰
            if new_page and old_page:
                if page_diff['char_diff'] >= 0 or page_diff['status'] == "improved":
                    selected_page = new_page
                    source = "new_ocr"
                else:
                    selected_page = old_page
                    source = "old_ocr"
            elif new_page:
                selected_page = new_page
                source = "new_ocr_only"
            elif old_page:
                selected_page = old_page
                source = "old_ocr_only"
            else:
                continue

            # çµ±åˆç‰ˆã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ
            unified_entry = {
                "pdf_index": pdf_idx,
                "page_number": page_num,
                "text": selected_page['text'],
                "timestamp": datetime.now().isoformat(),
                "unified_source": source,
                "old_char_count": len(old_page['text']) if old_page else 0,
                "new_char_count": len(new_page['text']) if new_page else 0
            }
            unified.append(unified_entry)

        logger.info(f"âœ… çµ±åˆç‰ˆä½œæˆå®Œäº†: {len(unified)}ãƒšãƒ¼ã‚¸")
        return unified

    def save_results(
        self,
        new_ocr: List[Dict],
        analysis: Dict,
        unified: List[Dict]
    ):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""

        # æ–°OCRçµæœ
        new_ocr_path = self.output_dir / f"ocr_results_deepseek_v2_{self.timestamp}.json"
        with open(new_ocr_path, 'w', encoding='utf-8') as f:
            json.dump(new_ocr, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… æ–°OCRçµæœä¿å­˜: {new_ocr_path}")

        # å·®åˆ†åˆ†æçµæœ
        analysis_path = self.output_dir / f"ocr_differential_analysis_{self.timestamp}.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… å·®åˆ†åˆ†æçµæœä¿å­˜: {analysis_path}")

        # çµ±åˆç‰ˆ
        unified_path = self.output_dir / f"ocr_results_unified_{self.timestamp}.json"
        with open(unified_path, 'w', encoding='utf-8') as f:
            json.dump(unified, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… çµ±åˆç‰ˆä¿å­˜: {unified_path}")

        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
        summary_report = self._generate_summary_report(analysis, new_ocr_path, analysis_path, unified_path)
        report_path = self.output_dir / f"ocr_reprocess_report_{self.timestamp}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        logger.info(f"âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

        return {
            "new_ocr": str(new_ocr_path),
            "analysis": str(analysis_path),
            "unified": str(unified_path),
            "report": str(report_path)
        }

    def _generate_summary_report(self, analysis: Dict, *file_paths: str) -> str:
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = f"""# è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆå†OCRå‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆ

**å‡¦ç†æ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## å‡¦ç†æ¦‚è¦

### å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
- â‘ .pdf: /mnt/c/Users/planj/Downloads/â‘ .pdf
- â‘¡.pdf: /mnt/c/Users/planj/Downloads/â‘¡.pdf
- â‘¢.pdf: /mnt/c/Users/planj/Downloads/â‘¢.pdf

### å‡¦ç†æ–¹æ³•
- **ã‚¨ãƒ³ã‚¸ãƒ³**: PyMuPDF (fitz) v1.23.x
- **æ–¹å¼**: ãƒšãƒ¼ã‚¸ã”ã¨ç‹¬ç«‹æŠ½å‡º + ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯æ•´å½¢
- **å“è³ª**: æ—§OCRã¨ã®æ¯”è¼ƒåˆ†æã«ã‚ˆã‚Šæœ€é©åŒ–

---

## å·®åˆ†åˆ†æçµæœ

### ãƒšãƒ¼ã‚¸æ•°çµ±è¨ˆ
- **æ—§OCRãƒšãƒ¼ã‚¸æ•°**: {analysis['old_ocr_count']}
- **æ–°OCRãƒšãƒ¼ã‚¸æ•°**: {analysis['new_ocr_count']}
- **æ¤œè¨¼ç·ãƒšãƒ¼ã‚¸æ•°**: {analysis['summary']['total_pages']}

### å“è³ªå¤‰åŒ–
| çŠ¶æ…‹ | ãƒšãƒ¼ã‚¸æ•° | èª¬æ˜ |
|------|--------|------|
| åŒä¸€ | {analysis['summary']['identical_pages']} | æ–°æ—§ã§åŒã˜å†…å®¹ |
| æ”¹å–„ | {analysis['summary']['improved_pages']} | æ–‡å­—æ•°ãŒ5%ä»¥ä¸Šå¢—åŠ  |
| å¤‰æ›´ | {analysis['summary']['degraded_pages']} | å†…å®¹ã«è‹¥å¹²ã®å¤‰åŒ– |
| æ–°è¦ | {analysis['summary']['missing_in_old']} | æ–°OCRã®ã¿ã«å­˜åœ¨ |
| å‰Šé™¤ | {analysis['summary']['missing_in_new']} | æ—§OCRã®ã¿ã«å­˜åœ¨ |

### æ–‡å­—æ•°çµ±è¨ˆ
- **æ—§OCRç·æ–‡å­—æ•°**: {analysis['quality_metrics']['old_total_chars']:,}å­—
- **æ–°OCRç·æ–‡å­—æ•°**: {analysis['quality_metrics']['new_total_chars']:,}å­—
- **å¤‰åŒ–ç‡**: {analysis['quality_metrics']['char_count_change_percent']:+.2f}%

---

## æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ

### æ¤œå‡ºã•ã‚ŒãŸæ”¹å–„ç®‡æ‰€
{self._generate_improvement_details(analysis)}

---

## å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«

1. **æ–°OCRçµæœ**: `ocr_results_deepseek_v2_{self.timestamp}.json`
   - æœ€æ–°æŠ½å‡ºã«ã‚ˆã‚‹å…¨ãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆ
   - å½¢å¼: æ—§OCRäº’æ›JSON

2. **å·®åˆ†åˆ†æ**: `ocr_differential_analysis_{self.timestamp}.json`
   - ãƒšãƒ¼ã‚¸åˆ¥ã®æ¯”è¼ƒçµæœ
   - å“è³ªæŒ‡æ¨™ã¨æ”¹å–„å†…å®¹

3. **çµ±åˆç‰ˆ**: `ocr_results_unified_{self.timestamp}.json`
   - æ–°æ—§ã®æœ€è‰¯éƒ¨åˆ†ã‚’çµ±åˆ
   - å•é¡Œç”Ÿæˆã«ä½¿ç”¨æ¨å¥¨

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **çµ±åˆç‰ˆã®æ¤œè¨¼**
   - `/data/ocr_results_unified_{self.timestamp}.json` ã‚’ç¢ºèª
   - ã‚µãƒ³ãƒ—ãƒ«ãƒšãƒ¼ã‚¸ã®å†…å®¹ãƒã‚§ãƒƒã‚¯

2. **ãƒ†ãƒ¼ãƒæŠ½å‡º**
   - çµ±åˆç‰ˆã‹ã‚‰ä¸»è¦ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º
   - ç²’åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆ1ã¤ã®ç‹¬ç«‹æ¦‚å¿µã‹ç¢ºèªï¼‰

3. **å•é¡Œç”Ÿæˆ**
   - æŠ½å‡ºã—ãŸãƒ†ãƒ¼ãƒã‚’åŸºã«12ãƒ‘ã‚¿ãƒ¼ãƒ³å±•é–‹
   - è¬›ç¿’å†…å®¹ã«åŸºã¥ã„ãŸæ­£ç¢ºãªå•é¡Œä½œæˆ

---

**å‡¦ç†å®Œäº†**: {datetime.now().isoformat()}
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: âœ… æˆåŠŸ

"""
        return report

    def _generate_improvement_details(self, analysis: Dict) -> str:
        """æ”¹å–„ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°ã‚’ç”Ÿæˆ"""
        improved_pages = [
            p for p in analysis['page_differences']
            if p['status'] == 'improved'
        ]

        if not improved_pages:
            return "- æ¤œå‡ºã•ã‚ŒãŸæ”¹å–„ãªã—ï¼ˆæ—¢å­˜OCRãŒæœ€é©åŒ–æ¸ˆã¿ï¼‰"

        details = []
        for page in improved_pages[:10]:  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
            char_increase = page['new_char_count'] - page['old_char_count']
            details.append(
                f"- PDF {page['pdf_index']} P.{page['page_number']}: "
                f"+{char_increase}å­— ({page['quality_change']})"
            )

        if len(improved_pages) > 10:
            details.append(f"- ãã®ä»– {len(improved_pages) - 10}ãƒšãƒ¼ã‚¸æ”¹å–„")

        return "\n".join(details) if details else "- æ”¹å–„ãƒšãƒ¼ã‚¸ãªã—"

    def run(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œ"""
        logger.info("=" * 70)
        logger.info("éŠæŠ€æ©Ÿå–æ‰±ä¸»ä»»è€… è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆ å†OCRå‡¦ç†é–‹å§‹")
        logger.info("=" * 70)

        # ã‚¹ãƒ†ãƒƒãƒ—1: æ—§OCRçµæœã‚’ãƒ­ãƒ¼ãƒ‰
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘æ—§OCRçµæœã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        old_ocr = self.load_old_ocr()
        if not old_ocr:
            logger.error("âŒ æ—§OCRçµæœã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—")
            return False

        # ã‚¹ãƒ†ãƒƒãƒ—2: æ–°OCRå‡¦ç†ï¼ˆå…¨3PDFï¼‰
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘æ–°OCRå‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
        new_ocr = self.reprocess_all_pdfs()
        if not new_ocr:
            logger.error("âŒ æ–°OCRå‡¦ç†ã«å¤±æ•—")
            return False

        # ã‚¹ãƒ†ãƒƒãƒ—3: å·®åˆ†åˆ†æ
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘å·®åˆ†åˆ†æã‚’å®Ÿè¡Œä¸­...")
        analysis = self.perform_differential_analysis(old_ocr, new_ocr)

        # ã‚¹ãƒ†ãƒƒãƒ—4: çµ±åˆç‰ˆä½œæˆ
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘çµ±åˆç‰ˆã‚’ä½œæˆä¸­...")
        unified = self.create_unified_ocr(old_ocr, new_ocr, analysis)

        # ã‚¹ãƒ†ãƒƒãƒ—5: çµæœä¿å­˜
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—5ã€‘çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ä¸­...")
        file_paths = self.save_results(new_ocr, analysis, unified)

        logger.info("\n" + "=" * 70)
        logger.info("âœ… å†OCRå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        logger.info("=" * 70)
        logger.info(f"\nå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
        for key, path in file_paths.items():
            logger.info(f"  - {key}: {path}")

        logger.info(f"\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ocr_results_unified_{self.timestamp}.json ã‚’ä½¿ç”¨ã—ã¦")
        logger.info(f"è¬›ç¿’å†…å®¹ã«åŸºã¥ã„ãŸå•é¡Œãƒ†ãƒ¼ãƒã‚’æŠ½å‡ºã—ã¦ãã ã•ã„")

        return True


def main():
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import argparse

    parser = argparse.ArgumentParser(
        description='éŠæŠ€æ©Ÿå–æ‰±ä¸»ä»»è€… è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆ å†OCRå‡¦ç†ï¼ˆåˆ†å‰²å‡¦ç†å¯¾å¿œç‰ˆï¼‰'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ãƒšãƒ¼ã‚¸ï¼‰'
    )
    parser.add_argument(
        '--resume',
        action='store_true',
        help='ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã™ã‚‹'
    )

    args = parser.parse_args()

    logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {args.batch_size}ãƒšãƒ¼ã‚¸")
    if args.resume:
        logger.info("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ãƒ¢ãƒ¼ãƒ‰")

    processor = LectureOCRReprocessor(batch_size=args.batch_size)
    success = processor.run()
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
