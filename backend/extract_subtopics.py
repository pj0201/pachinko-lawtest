#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ

éŠæŠ€æ©Ÿå–æ‰±ä¸»ä»»è€…è©¦é¨“ã®OCRãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€
ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’æ§‹é€ åŒ–æŠ½å‡ºã™ã‚‹
"""

import json
import re
from collections import defaultdict

def load_ocr_data(ocr_path):
    """OCRãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    with open(ocr_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_wind_eikyo(md_path):
    """é¢¨å–¶æ³•MDã‚’èª­ã¿è¾¼ã‚€"""
    with open(md_path, 'r', encoding='utf-8') as f:
        return f.read()

def extract_chapters_from_ocr(ocr_data):
    """OCRã‹ã‚‰ç« ãƒ»ç¯€æ§‹é€ ã‚’æŠ½å‡º"""
    chapters = defaultdict(lambda: {"sections": [], "raw_text": ""})
    current_chapter = None

    for page in ocr_data:
        text = page.get('text', '')
        page_num = page.get('page_number', 0)

        # ç« ã®æ¤œå‡ºï¼ˆç¬¬Xç« ï¼‰
        chapter_matches = re.finditer(r'ç¬¬([1-9])ç« \s+([^\n]+)', text)
        for match in chapter_matches:
            current_chapter = match.group(1)
            ch_title = match.group(2).strip()
            chapters[current_chapter]["title"] = ch_title
            chapters[current_chapter]["page_start"] = page_num

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ¤œå‡ºï¼ˆ(1), (2), ...ï¼‰
        if current_chapter:
            section_matches = re.finditer(r'\(([0-9â‘ â‘¡â‘¢â‘£â‘¤]+)\)\s*([^\n]+)', text)
            for match in section_matches:
                sec_num = match.group(1)
                sec_title = match.group(2).strip()
                chapters[current_chapter]["sections"].append({
                    "number": sec_num,
                    "title": sec_title,
                    "page": page_num
                })

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’è“„ç©
        if current_chapter:
            chapters[current_chapter]["raw_text"] += text + "\n"

    return chapters

def extract_wind_eikyo_sections(md_text):
    """é¢¨å–¶æ³•ã‹ã‚‰ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º"""
    topics = []

    # ## ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
    matches = re.finditer(r'^## (.+)$', md_text, re.MULTILINE)
    for match in matches:
        topics.append(match.group(1).strip())

    # ### ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚‚æŠ½å‡º
    sub_matches = re.finditer(r'^### (.+)$', md_text, re.MULTILINE)
    for match in sub_matches:
        topics.append("â””â”€ " + match.group(1).strip())

    return topics

def map_to_exam_categories():
    """
    éŠæŠ€æ©Ÿå–æ‰±ä¸»ä»»è€…è©¦é¨“ã®ã‚«ãƒ†ã‚´ãƒªã«ãƒãƒƒãƒ”ãƒ³ã‚°

    7ã¤ã®ã‚«ãƒ†ã‚´ãƒªï¼š
    1. å–¶æ¥­è¨±å¯ãƒ»ç”³è«‹æ‰‹ç¶šã
    2. å–¶æ¥­æ™‚é–“ãƒ»å–¶æ¥­å ´æ‰€
    3. éŠæŠ€æ©Ÿè¦åˆ¶
    4. å¾“æ¥­è€…ã®è¦ä»¶ãƒ»ç¦æ­¢äº‹é …
    5. é¡§å®¢ä¿è­·ãƒ»è¦åˆ¶éµå®ˆ
    6. æ³•ä»¤é•åã¨è¡Œæ”¿å‡¦åˆ†
    7. å®Ÿå‹™çš„å¯¾å¿œ
    """
    categories = {
        "permits": {
            "id": "permits",
            "name": "å–¶æ¥­è¨±å¯ãƒ»ç”³è«‹æ‰‹ç¶šã",
            "keywords": ["è¨±å¯", "ç”³è«‹", "å±Šå‡º", "å–¶æ¥­", "è¦ä»¶"],
            "subtopics": []
        },
        "business_hours": {
            "id": "business_hours",
            "name": "å–¶æ¥­æ™‚é–“ãƒ»å–¶æ¥­å ´æ‰€",
            "keywords": ["å–¶æ¥­æ™‚é–“", "å–¶æ¥­å ´æ‰€", "æ–½è¨­", "åŸºæº–", "æ§‹é€ "],
            "subtopics": []
        },
        "gaming_machines": {
            "id": "gaming_machines",
            "name": "éŠæŠ€æ©Ÿè¦åˆ¶",
            "keywords": ["éŠæŠ€æ©Ÿ", "æ¤œå®š", "æ”¹é€ ", "æ¤œæŸ»", "åŸºæº–"],
            "subtopics": []
        },
        "employees": {
            "id": "employees",
            "name": "å¾“æ¥­è€…ã®è¦ä»¶ãƒ»ç¦æ­¢äº‹é …",
            "keywords": ["ä¸»ä»»è€…", "å¾“æ¥­å“¡", "è³‡æ ¼", "ç¦æ­¢", "é›‡ç”¨"],
            "subtopics": []
        },
        "customer_protection": {
            "id": "customer_protection",
            "name": "é¡§å®¢ä¿è­·ãƒ»è¦åˆ¶éµå®ˆ",
            "keywords": ["é¡§å®¢", "æœªæˆå¹´", "æ™¯å“", "äº¤æ›", "ä¿è­·"],
            "subtopics": []
        },
        "violations": {
            "id": "violations",
            "name": "æ³•ä»¤é•åã¨è¡Œæ”¿å‡¦åˆ†",
            "keywords": ["é•å", "å‡¦åˆ†", "åœæ­¢", "å–æ¶ˆ", "è¡Œæ”¿"],
            "subtopics": []
        },
        "practical": {
            "id": "practical",
            "name": "å®Ÿå‹™çš„å¯¾å¿œ",
            "keywords": ["å¯¾å¿œ", "å ±å‘Š", "è¨˜éŒ²", "ç®¡ç†", "å®Ÿå‹™"],
            "subtopics": []
        }
    }

    return categories

def extract_subtopics_from_text(text, keywords):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ãã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
    """
    subtopics = []

    # æ–‡ã”ã¨ã«å‡¦ç†
    sentences = re.split(r'[ã€‚ï¼\n]+', text)

    for sentence in sentences:
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡ã‚’æŠ½å‡º
        for keyword in keywords:
            if keyword in sentence and len(sentence) > 10:
                # é‡è¤‡ã‚’é¿ã‘ã‚‹
                if sentence.strip() not in [st["text"] for st in subtopics]:
                    subtopics.append({
                        "text": sentence.strip()[:100],  # æœ€åˆã®100æ–‡å­—
                        "keyword": keyword
                    })
                break

        if len(subtopics) >= 5:  # ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šæœ€å¤§5ãƒˆãƒ”ãƒƒã‚¯
            break

    return subtopics

def main():
    print("ğŸš€ ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºé–‹å§‹\n")

    # ã‚½ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã‚€
    ocr_path = '/home/planj/patshinko-exam-app/data/ocr_results_corrected.json'
    wind_path = '/home/planj/Claude-Code-Communication/resources/legal/wind_eikyo_law/wind_eikyo_law_v1.0.md'

    print("ğŸ“– ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    ocr_data = load_ocr_data(ocr_path)
    wind_text = load_wind_eikyo(wind_path)

    print(f"âœ… OCR: {len(ocr_data)}ãƒšãƒ¼ã‚¸")
    print(f"âœ… é¢¨å–¶æ³•: {len(wind_text)}æ–‡å­—\n")

    # OCRã‹ã‚‰ç« æ§‹é€ ã‚’æŠ½å‡º
    print("ğŸ“‹ OCRã‹ã‚‰ç« ãƒ»ç¯€æ§‹é€ ã‚’æŠ½å‡º...")
    chapters = extract_chapters_from_ocr(ocr_data)
    for ch_num, ch_data in sorted(chapters.items()):
        if "title" in ch_data:
            print(f"  ç¬¬{ch_num}ç« : {ch_data['title']}")
            for section in ch_data.get("sections", []):
                print(f"    ({section['number']}) {section['title']}")

    # ã‚«ãƒ†ã‚´ãƒªã‚’åˆæœŸåŒ–
    print("\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªã«ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°...")
    categories = map_to_exam_categories()

    # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é›†ç´„
    all_text = "\n".join([ch["raw_text"] for ch in chapters.values()])
    all_text += "\n" + wind_text

    # ã‚µãƒ–ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
    for cat_id, cat_data in categories.items():
        subtopics = extract_subtopics_from_text(all_text, cat_data["keywords"])
        cat_data["subtopics"] = subtopics
        print(f"  {cat_data['name']}: {len(subtopics)}ãƒˆãƒ”ãƒƒã‚¯")

    # çµæœã‚’ä¿å­˜
    output = {
        "generated_at": "2025-10-22",
        "source": {
            "ocr_pages": len(ocr_data),
            "wind_eikyo_chars": len(wind_text)
        },
        "categories": categories
    }

    output_path = '/home/planj/patshinko-exam-app/backend/PATSHINKO_EXAM_STRUCTURE.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å‡ºåŠ›: {output_path}")
    print("\nã€æŠ½å‡ºçµæœã‚µãƒãƒªãƒ¼ã€‘")
    for cat in categories.values():
        print(f"  {cat['name']}: {len(cat['subtopics'])}ãƒˆãƒ”ãƒƒã‚¯")

if __name__ == '__main__':
    main()
