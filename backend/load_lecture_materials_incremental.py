#!/usr/bin/env python3
"""
è¬›ç¾©è³‡æ–™ã®æ®µéšçš„RAGèª­ã¿è¾¼ã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
OCRçµæœã‚’åˆ†å‰²ã—ã¦ChromaDBã«æ®µéšçš„ã«èª­ã¿è¾¼ã¿ã¾ã™
"""

import json
import sys
import time
from pathlib import Path
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent))

try:
    from text_chunker import TextChunker
    from chroma_rag import ChromaRAG
except ImportError as e:
    print(f"âš ï¸ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    print("æ³¨: ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Node.jsç’°å¢ƒã®ãŸã‚ã€ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½¿ç”¨ã—ã¾ã™")


class IncrementalRAGLoader:
    """è¬›ç¾©è³‡æ–™ã®æ®µéšçš„èª­ã¿è¾¼ã¿"""

    def __init__(self, ocr_path: str, batch_size: int = 20):
        """
        Args:
            ocr_path: OCRçµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            batch_size: ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°
        """
        self.ocr_path = ocr_path
        self.batch_size = batch_size
        self.checkpoint_file = Path(__file__).parent / "data" / "rag_loading_checkpoint.json"

    def load_ocr_data(self):
        """OCRçµæœã‚’èª­ã¿è¾¼ã‚€"""
        print(f"ğŸ“– OCRçµæœã‚’èª­ã¿è¾¼ã¿ä¸­: {self.ocr_path}")
        with open(self.ocr_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # PDFåˆ¥ã«åˆ†é¡
        pdf_data = {1: [], 2: [], 3: []}
        for page in data:
            pdf_idx = page.get('pdf_index', 1)
            if pdf_idx in pdf_data:
                pdf_data[pdf_idx].append(page)

        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:")
        for pdf_idx, pages in pdf_data.items():
            print(f"   PDF {pdf_idx}: {len(pages)}ãƒšãƒ¼ã‚¸")

        return pdf_data

    def load_checkpoint(self):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
        if self.checkpoint_file.exists():
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"pdf_index": 1, "page_index": 0, "processed_pages": 0}

    def save_checkpoint(self, pdf_index: int, page_index: int, processed_pages: int):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜"""
        self.checkpoint_file.parent.mkdir(parents=True, exist_ok=True)
        checkpoint = {
            "pdf_index": pdf_index,
            "page_index": page_index,
            "processed_pages": processed_pages,
            "timestamp": datetime.now().isoformat()
        }
        with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: PDF{pdf_index}, ãƒšãƒ¼ã‚¸{page_index}, ç´¯è¨ˆ{processed_pages}ãƒšãƒ¼ã‚¸")

    def process_batch(self, pages_batch, pdf_index: int, start_page: int):
        """ãƒãƒƒãƒå‡¦ç†"""
        print(f"\nğŸ“ å‡¦ç†ä¸­: PDF{pdf_index}, ãƒšãƒ¼ã‚¸ {start_page+1}ã€œ{start_page+len(pages_batch)}")

        # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯åŒ–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        # å®Ÿéš›ã®ChromaDBå‡¦ç†ã¯Node.jsç’°å¢ƒã§è¡Œã†å¿…è¦ãŒã‚ã‚Šã¾ã™
        chunks = []
        for page in pages_batch:
            # ç°¡å˜ãªãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆå®Ÿéš›ã¯text_chunkerã‚’ä½¿ç”¨ï¼‰
            text = page.get('text', '')
            if len(text) > 800:
                # 800æ–‡å­—ãšã¤ã«åˆ†å‰²
                for i in range(0, len(text), 800):
                    chunk_text = text[i:i+800]
                    chunks.append({
                        "text": chunk_text,
                        "pdf_index": pdf_index,
                        "page_number": page.get('page_number'),
                        "source": "lecture_materials"
                    })
            else:
                chunks.append({
                    "text": text,
                    "pdf_index": pdf_index,
                    "page_number": page.get('page_number'),
                    "source": "lecture_materials"
                })

        print(f"   âœ… {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ")
        return chunks

    def run_incremental_loading(self):
        """æ®µéšçš„èª­ã¿è¾¼ã¿ã®å®Ÿè¡Œ"""
        print("=" * 70)
        print("ğŸš€ è¬›ç¾©è³‡æ–™ã®æ®µéšçš„RAGèª­ã¿è¾¼ã¿ã‚’é–‹å§‹")
        print("=" * 70)

        # OCRãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        pdf_data = self.load_ocr_data()

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿
        checkpoint = self.load_checkpoint()
        print(f"\nğŸ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹: PDF{checkpoint['pdf_index']}, ãƒšãƒ¼ã‚¸{checkpoint['page_index']}")

        total_chunks = 0

        # PDFåˆ¥ã«å‡¦ç†
        for pdf_idx in [1, 2, 3]:
            if pdf_idx < checkpoint['pdf_index']:
                print(f"\nâ­ï¸  PDF{pdf_idx}ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†æ¸ˆã¿ï¼‰")
                continue

            pages = pdf_data[pdf_idx]
            start_idx = checkpoint['page_index'] if pdf_idx == checkpoint['pdf_index'] else 0

            print(f"\nğŸ“„ PDF{pdf_idx}ã‚’å‡¦ç†ä¸­ ({len(pages)}ãƒšãƒ¼ã‚¸)")

            # ãƒãƒƒãƒå‡¦ç†
            for i in range(start_idx, len(pages), self.batch_size):
                batch = pages[i:i+self.batch_size]
                chunks = self.process_batch(batch, pdf_idx, i)
                total_chunks += len(chunks)

                # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆå®Ÿéš›ã®ChromaDBã¸ã®è¿½åŠ ã®ä»£ã‚ã‚Šï¼‰
                self._save_chunks_to_file(chunks, pdf_idx, i)

                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                self.save_checkpoint(pdf_idx, i + len(batch), checkpoint['processed_pages'] + len(batch))
                checkpoint['processed_pages'] += len(batch)

                # çŸ­ã„ä¼‘æ†©ï¼ˆãƒ¡ãƒ¢ãƒªç®¡ç†ï¼‰
                time.sleep(0.5)

        print("\n" + "=" * 70)
        print("âœ… æ®µéšçš„èª­ã¿è¾¼ã¿å®Œäº†")
        print(f"   ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {total_chunks}")
        print(f"   ç·å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {checkpoint['processed_pages']}")
        print("=" * 70)

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚¯ãƒªã‚¢
        if self.checkpoint_file.exists():
            self.checkpoint_file.unlink()

        return True

    def _save_chunks_to_file(self, chunks, pdf_index: int, page_index: int):
        """ãƒãƒ£ãƒ³ã‚¯ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output_dir = Path(__file__).parent / "data" / "chunks"
        output_dir.mkdir(parents=True, exist_ok=True)

        filename = f"chunks_pdf{pdf_index}_p{page_index:04d}.json"
        output_path = output_dir / filename

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)

        print(f"   ğŸ’¾ ãƒãƒ£ãƒ³ã‚¯ä¿å­˜: {filename}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # OCRçµæœã®ãƒ‘ã‚¹
    ocr_path = "/home/user/pachinko-lawtest/data/old_problems/ocr_results_corrected.json"

    if not Path(ocr_path).exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: OCRçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ocr_path}")
        return 1

    # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ãƒšãƒ¼ã‚¸ãšã¤ï¼‰
    batch_size = 20

    if len(sys.argv) > 1:
        try:
            batch_size = int(sys.argv[1])
            print(f"ğŸ“Œ ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}ãƒšãƒ¼ã‚¸")
        except ValueError:
            print("âš ï¸ ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯æ•´æ•°ã§æŒ‡å®šã—ã¦ãã ã•ã„")

    loader = IncrementalRAGLoader(ocr_path, batch_size)

    try:
        success = loader.run_incremental_loading()
        return 0 if success else 1
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
