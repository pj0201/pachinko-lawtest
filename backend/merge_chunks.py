#!/usr/bin/env python3
"""
ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
from pathlib import Path
from datetime import datetime


def merge_chunks():
    """å…¨ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ"""
    print("=" * 70)
    print("ğŸ”„ ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆã‚’é–‹å§‹")
    print("=" * 70)

    chunks_dir = Path(__file__).parent / "data" / "chunks"
    output_file = Path(__file__).parent / "data" / "lecture_materials_chunks.json"

    if not chunks_dir.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {chunks_dir}")
        return False

    # å…¨ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    all_chunks = []
    chunk_files = sorted(chunks_dir.glob("chunks_pdf*.json"))

    print(f"\nğŸ“‚ {len(chunk_files)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹")

    for chunk_file in chunk_files:
        with open(chunk_file, 'r', encoding='utf-8') as f:
            chunks = json.load(f)
            all_chunks.extend(chunks)
            print(f"   âœ… {chunk_file.name}: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯")

    # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_data = {
        "metadata": {
            "total_chunks": len(all_chunks),
            "source": "lecture_materials_ocr",
            "created_at": datetime.now().isoformat(),
            "pdf_count": 3,
            "total_pages": 220
        },
        "chunks": all_chunks
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {output_file}")
    print(f"   ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {len(all_chunks)}")
    print("\n" + "=" * 70)
    print("âœ… ãƒãƒ£ãƒ³ã‚¯çµ±åˆå®Œäº†")
    print("=" * 70)

    return True


if __name__ == "__main__":
    import sys
    success = merge_chunks()
    sys.exit(0 if success else 1)
