#!/usr/bin/env python3
"""
è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆ OCRåˆ†æãƒ»æº–å‚™ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
========================================

ç›®çš„:
  æ—¢å­˜OCRã®å“è³ªåˆ†æ
  å•é¡Œç”Ÿæˆã«å‘ã‘ãŸãƒ†ãƒ¼ãƒæŠ½å‡ºã®æº–å‚™
  æ¤œè¨¼å¯èƒ½ãªè¬›ç¿’å†…å®¹ã®æ§‹ç¯‰

æˆ¦ç•¥:
  ç¾åœ¨ã®OCRã¯ã‚¹ã‚­ãƒ£ãƒ³ç”»åƒãƒ™ãƒ¼ã‚¹ã®ãŸã‚ã€å†OCRã¯é›£ã—ã„
  â†’ æ—¢å­˜OCRã®å“è³ªåˆ†æã¨æ§‹é€ åŒ–ã‚’å®Ÿæ–½
  â†’ è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆã®é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰¹å®š
  â†’ ãƒ†ãƒ¼ãƒæŠ½å‡ºç”¨ã®åŸºç›¤ã‚’æº–å‚™
"""

import json
import sys
import logging
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime
from collections import defaultdict
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LectureOCRAnalyzer:
    """è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆOCRåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        self.ocr_path = "/home/planj/patshinko-exam-app/data/ocr_results_corrected.json"
        self.output_dir = Path("/home/planj/patshinko-exam-app/data")
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    def load_ocr(self) -> List[Dict]:
        """OCRçµæœã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            with open(self.ocr_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"OCRãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return []

    def analyze_ocr_quality(self, ocr_data: List[Dict]) -> Dict:
        """OCRå“è³ªã‚’åˆ†æ"""
        logger.info("OCRå“è³ªåˆ†æé–‹å§‹...")

        analysis = {
            "timestamp": datetime.now().isoformat(),
            "total_pages": len(ocr_data),
            "summary": {
                "total_pages": len(ocr_data)
            },
            "quality_metrics": {
                "total_characters": 0,
                "average_chars_per_page": 0,
                "pdf_distribution": {1: 0, 2: 0, 3: 0},
                "pages_by_pdf": {1: 0, 2: 0, 3: 0}
            },
            "content_analysis": {
                "chapters": [],
                "sections": [],
                "tables": [],
                "lists": [],
                "special_sections": []
            },
            "quality_flags": {
                "empty_pages": [],
                "suspicious_content": [],
                "possible_errors": []
            }
        }

        # ãƒšãƒ¼ã‚¸ã”ã¨åˆ†æ
        for page in ocr_data:
            pdf_idx = page['pdf_index']
            page_num = page['page_number']
            text = page['text']

            # çµ±è¨ˆ
            char_count = len(text)
            analysis['quality_metrics']['total_characters'] += char_count
            analysis['quality_metrics']['pages_by_pdf'][pdf_idx] += 1

            # ç©ºãƒšãƒ¼ã‚¸ãƒã‚§ãƒƒã‚¯
            if char_count < 50:
                analysis['quality_flags']['empty_pages'].append(f"PDF{pdf_idx}-P{page_num}")

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†é¡
            if re.search(r'^(ç¬¬[0-9ï¼-ï¼™]+ç« |ç¬¬[0-9ï¼-ï¼™]+é …)', text):
                analysis['content_analysis']['chapters'].append({
                    'pdf': pdf_idx,
                    'page': page_num,
                    'preview': text[:100]
                })

            if re.search(r'ï¼ˆ[0-9ï¼-ï¼™]+ï¼‰|[0-9ï¼-ï¼™]+\)|â‘ |â‘¡|â‘¢|â‘£|â‘¤', text[:200]):
                analysis['content_analysis']['lists'].append({
                    'pdf': pdf_idx,
                    'page': page_num
                })

            # OCRã‚¨ãƒ©ãƒ¼ã®ãƒªã‚¹ã‚¯æ¤œå‡º
            if re.search(r'[ã‚¡-ãƒ´ãƒ¼]{50,}', text):  # ç•°å¸¸ãªã‚«ã‚¿ã‚«ãƒŠé€£ç¶š
                analysis['quality_flags']['possible_errors'].append(f"PDF{pdf_idx}-P{page_num}: ç•°å¸¸ãªã‚«ã‚¿ã‚«ãƒŠ")

            if 'â–¡' in text or 'â–³' in text or 'â—†' in text:
                if 'â–¡' in text:
                    analysis['content_analysis']['tables'].append({
                        'pdf': pdf_idx,
                        'page': page_num,
                        'has_boxes': True
                    })

        # çµ±è¨ˆè¨ˆç®—
        if analysis['quality_metrics']['total_characters'] > 0:
            analysis['quality_metrics']['average_chars_per_page'] = round(
                analysis['quality_metrics']['total_characters'] /
                analysis['total_pages']
            )

        logger.info(f"âœ… å“è³ªåˆ†æå®Œäº†")
        logger.info(f"  ç·æ–‡å­—æ•°: {analysis['quality_metrics']['total_characters']:,}å­—")
        logger.info(f"  å¹³å‡ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {analysis['quality_metrics']['average_chars_per_page']}å­—")
        logger.info(f"  ç©ºãƒšãƒ¼ã‚¸: {len(analysis['quality_flags']['empty_pages'])}ãƒšãƒ¼ã‚¸")
        logger.info(f"  å¯èƒ½ãªOCRã‚¨ãƒ©ãƒ¼: {len(analysis['quality_flags']['possible_errors'])}ç®‡æ‰€")

        return analysis

    def extract_key_sections(self, ocr_data: List[Dict]) -> Dict:
        """é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
        logger.info("é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡ºé–‹å§‹...")

        sections = {
            "intro": [],
            "chapter1": [],  # éŠæŠ€æ©Ÿå–æ‰±ä¸»ä»»è€…åˆ¶åº¦
            "chapter2": [],  # éŠæŠ€å ´å–¶æ¥­è¦åˆ¶
            "chapter3": [],  # å®Ÿå‹™
            "reference": [],
            "identified_themes": []
        }

        for page in ocr_data:
            text = page['text']
            pdf_idx = page['pdf_index']
            page_num = page['page_number']

            page_ref = f"PDF{pdf_idx}-P{page_num}"

            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†é¡
            if page_num <= 15:
                sections['intro'].append({
                    'page': page_ref,
                    'preview': text[:150]
                })

            # ãƒ†ãƒ¼ãƒæŠ½å‡ºå€™è£œã‚’æ¤œå‡º
            # å–¶æ¥­è¨±å¯é–¢é€£
            if 'å–¶æ¥­è¨±å¯' in text:
                self._extract_theme_candidate(
                    sections['identified_themes'],
                    "å–¶æ¥­è¨±å¯é–¢é€£", page_ref, text
                )

            # å‹å¼æ¤œå®šé–¢é€£
            if 'å‹å¼æ¤œå®š' in text or 'å‹å¼æ¤œæŸ»' in text:
                self._extract_theme_candidate(
                    sections['identified_themes'],
                    "å‹å¼æ¤œå®šé–¢é€£", page_ref, text
                )

            # éŠæŠ€æ©Ÿé–¢é€£
            if 'éŠæŠ€æ©Ÿ' in text and ('è¨­ç½®' in text or 'æ–°å°' in text or 'ä¸­å¤' in text):
                self._extract_theme_candidate(
                    sections['identified_themes'],
                    "éŠæŠ€æ©Ÿç®¡ç†", page_ref, text
                )

            # å–¶æ¥­æ™‚é–“ãƒ»å–¶æ¥­ç¦æ­¢
            if 'å–¶æ¥­æ™‚é–“' in text or 'å–¶æ¥­ç¦æ­¢' in text:
                self._extract_theme_candidate(
                    sections['identified_themes'],
                    "å–¶æ¥­æ™‚é–“ãƒ»è¦åˆ¶", page_ref, text
                )

            # æ™¯å“é–¢é€£
            if 'æ™¯å“' in text and ('ç¨®é¡' in text or 'é™å®š' in text or 'å“ç›®' in text):
                self._extract_theme_candidate(
                    sections['identified_themes'],
                    "æ™¯å“è¦åˆ¶", page_ref, text
                )

            # ä¸æ­£å¯¾ç­–
            if 'ä¸æ­£' in text and ('é˜²æ­¢' in text or 'å¯¾ç­–' in text):
                self._extract_theme_candidate(
                    sections['identified_themes'],
                    "ä¸æ­£å¯¾ç­–", page_ref, text
                )

        logger.info(f"âœ… ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡ºå®Œäº†")
        logger.info(f"  æŠ½å‡ºã•ã‚ŒãŸãƒ†ãƒ¼ãƒå€™è£œ: {len(sections['identified_themes'])}ä»¶")

        return sections

    def _extract_theme_candidate(self, themes_list: List, category: str, page_ref: str, text: str):
        """ãƒ†ãƒ¼ãƒå€™è£œã‚’æŠ½å‡ºï¼ˆé‡è¤‡å›é¿ï¼‰"""
        candidate = {
            'category': category,
            'page': page_ref,
            'text_preview': text[:200]
        }

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        existing = [t for t in themes_list if t.get('category') == category and t.get('page') == page_ref]
        if not existing:
            themes_list.append(candidate)

    def generate_preparation_guide(self, analysis: Dict, sections: Dict) -> str:
        """ãƒ†ãƒ¼ãƒæŠ½å‡ºæº–å‚™ã‚¬ã‚¤ãƒ‰ã‚’ç”Ÿæˆ"""
        guide = f"""# è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆåˆ†æçµæœ & ãƒ†ãƒ¼ãƒæŠ½å‡ºæº–å‚™ã‚¬ã‚¤ãƒ‰

**åˆ†ææ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

---

## ğŸ“Š OCRå“è³ªåˆ†æ

### åŸºæœ¬çµ±è¨ˆ
- **ç·ãƒšãƒ¼ã‚¸æ•°**: {analysis['total_pages']}ãƒšãƒ¼ã‚¸
- **ç·æ–‡å­—æ•°**: {analysis['quality_metrics']['total_characters']:,}å­—
- **å¹³å‡ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º**: {analysis['quality_metrics']['average_chars_per_page']}å­—/ãƒšãƒ¼ã‚¸

### ãƒšãƒ¼ã‚¸åˆ†å¸ƒ
"""
        for pdf_idx in [1, 2, 3]:
            pages = analysis['quality_metrics']['pages_by_pdf'].get(pdf_idx, 0)
            guide += f"- PDF {pdf_idx}: {pages}ãƒšãƒ¼ã‚¸\n"

        guide += f"""

### å“è³ªãƒã‚§ãƒƒã‚¯çµæœ
- **ç©ºç™½ãƒšãƒ¼ã‚¸**: {len(analysis.get('quality_flags', {}).get('empty_pages', []))}ãƒšãƒ¼ã‚¸
  {chr(10).join([f"  - {p}" for p in analysis.get('quality_flags', {}).get('empty_pages', [])[:5]]) if analysis.get('quality_flags', {}).get('empty_pages') else "  (ãªã—)"}

- **å¯èƒ½ãªOCRã‚¨ãƒ©ãƒ¼**: {len(analysis.get('quality_flags', {}).get('possible_errors', []))}ç®‡æ‰€
  {chr(10).join([f"  - {p}" for p in analysis.get('quality_flags', {}).get('possible_errors', [])[:5]]) if analysis.get('quality_flags', {}).get('possible_errors') else "  (ãªã—)"}

### ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹é€ 
- **ç« ç«‹ã¦**: {len(analysis['content_analysis']['chapters'])}ç« æ¤œå‡º
- **ãƒªã‚¹ãƒˆæ§‹é€ **: {len(analysis['content_analysis']['lists'])}ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ¤œå‡º
- **è¡¨ãƒ»ãƒœãƒƒã‚¯ã‚¹**: {len(analysis['content_analysis']['tables'])}å€‹æ¤œå‡º

---

## ğŸ¯ æŠ½å‡ºã•ã‚ŒãŸãƒ†ãƒ¼ãƒå€™è£œ

ç¾åœ¨ã®è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒå€™è£œã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚
ã“ã‚Œã‚‰ã¯12ãƒ‘ã‚¿ãƒ¼ãƒ³å•é¡Œå±•é–‹ã®åŸºç¤ã¨ãªã‚Šã¾ã™ã€‚

### ãƒ†ãƒ¼ãƒåˆ¥åˆ†å¸ƒ
"""

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
        category_counts = defaultdict(int)
        for theme in sections['identified_themes']:
            category_counts[theme['category']] += 1

        for category, count in sorted(category_counts.items(), key=lambda x: -x[1]):
            guide += f"- **{category}**: {count}ä»¶\n"

        guide += f"""

### æ¤œå‡ºãƒ†ãƒ¼ãƒã®è©³ç´°

"""
        for i, theme in enumerate(sections['identified_themes'][:20], 1):
            guide += f"""#### {i}. {theme['category']} ({theme['page']})
ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:
```
{theme['text_preview'][:150]}...
```

"""

        if len(sections['identified_themes']) > 20:
            guide += f"*ä»– {len(sections['identified_themes']) - 20}ä»¶ã®ãƒ†ãƒ¼ãƒå€™è£œãŒæ¤œå‡ºã•ã‚Œã¦ã„ã¾ã™*\n\n"

        guide += """---

## ğŸ“‹ ãƒ†ãƒ¼ãƒæŠ½å‡ºãƒ•ãƒ­ãƒ¼

### ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ãƒ¼ãƒã®ç²’åº¦ç¢ºèª
å„ãƒ†ãƒ¼ãƒãŒã€Œ1ã¤ã®ç‹¬ç«‹ã—ãŸæ³•å¾‹æ¦‚å¿µã€ã‹æ¤œè¨¼ï¼š
- ãƒ†ãƒ¼ãƒèª¬æ˜ãŒ1ï½2æ–‡ã§å®Œçµã™ã‚‹ã‹
- ãƒ†ãƒ¼ãƒãŒé¢¨å–¶æ³•ã®ã©ã®æ¡é …ã«åŸºã¥ãã‹
- ãƒ†ãƒ¼ãƒã‹ã‚‰è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å•é¡ŒãŒæ´¾ç”Ÿå¯èƒ½ã‹

**ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
- [ ] ãƒ†ãƒ¼ãƒãŒè¤‡æ•°ã®æ¦‚å¿µã‚’å«ã‚“ã§ã„ãªã„ã‹
- [ ] ãƒ†ãƒ¼ãƒãŒã€Œå®Ÿå‹™çš„ãªåˆ¤å®šã€ã‚’å«ã‚“ã§ã„ã‚‹ã‹
- [ ] ãƒ†ãƒ¼ãƒãŒè¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆã«æ ¹æ‹ ã‚’æŒã¤ã‹

### ã‚¹ãƒ†ãƒƒãƒ—2: è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®å…·ä½“çš„ã‚·ãƒŠãƒªã‚ªæŠ½å‡º
å„ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆã«è¨˜è¼‰ã•ã‚ŒãŸå®Ÿä¾‹ã‚’æŠ½å‡ºï¼š
- ã€Œï½ã®å ´åˆã€ã¨ã„ã†æ¡ä»¶æ–‡
- ã€Œé•åã€ã€Œç¦æ­¢ã€ã€Œå¯èƒ½ã€ãªã©ã®åˆ¤å®šçµæœ
- å…·ä½“çš„ãªæ™‚é–“æœŸé™ã‚„æ•°å€¤

### ã‚¹ãƒ†ãƒƒãƒ—3: 12ãƒ‘ã‚¿ãƒ¼ãƒ³ã¸ã®å±•é–‹
å„ãƒ†ãƒ¼ãƒã‚’12ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å±•é–‹ï¼š
1. åŸºæœ¬çŸ¥è­˜
2. ã²ã£ã‹ã‘ï¼ˆçµ¶å¯¾è¡¨ç¾ï¼‰
3. ç”¨èªæ¯”è¼ƒ
4. å„ªå…ˆé †ä½
5. æ™‚ç³»åˆ—ç†è§£
6. ã‚·ãƒŠãƒªã‚ªåˆ¤å®š
7. è¤‡åˆé•å
8. æ•°å€¤æ­£ç¢ºæ€§
9. ç†ç”±ç†è§£
10. çµŒé¨“é™¥é˜±
11. æ³•å¾‹ç¢ºèª
12. è¤‡åˆå¿œç”¨

### ã‚¹ãƒ†ãƒƒãƒ—4: æ ¹æ‹ æ–‡çŒ®ã®è¨˜éŒ²
å„å•é¡Œã«ã¤ã„ã¦ã€è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆã®ã©ã®ãƒšãƒ¼ã‚¸ã‚’æ ¹æ‹ ã¨ã—ã¦ã„ã‚‹ã‹è¨˜éŒ²

---

## ğŸ” æ¨å¥¨ã•ã‚Œã‚‹æ¤œè¨¼ä½œæ¥­

### å„ªå…ˆåº¦ãŒé«˜ã„ãƒ†ãƒ¼ãƒï¼ˆæ—¢ã«æ¤œå‡ºæ¸ˆã¿ï¼‰
"""

        # æœ€å„ªå…ˆãƒ†ãƒ¼ãƒ
        priority_themes = [
            ("å–¶æ¥­è¨±å¯é–¢é€£", "å–¶æ¥­è¨±å¯ã®æœ‰åŠ¹æœŸé™ã¯ç„¡æœŸé™ã‹3å¹´ã‹"),
            ("å‹å¼æ¤œå®šé–¢é€£", "å‹å¼æ¤œå®šã®æœ‰åŠ¹æœŸé™ã¨æ›´æ–°æ¡ä»¶"),
            ("éŠæŠ€æ©Ÿç®¡ç†", "æ–°å°ãƒ»ä¸­å¤è¨­ç½®ã®æ‰‹ç¶šã"),
            ("å–¶æ¥­æ™‚é–“ãƒ»è¦åˆ¶", "å–¶æ¥­ç¦æ­¢æ™‚é–“å¸¯ã¨å–¶æ¥­æ™‚é–“"),
            ("æ™¯å“è¦åˆ¶", "æ™¯å“ã®ç¨®é¡ãƒ»å“ç›®ãƒ»é‡‘é¡åˆ¶é™"),
            ("ä¸æ­£å¯¾ç­–", "ä¸æ­£æ”¹é€ ã¨å¯¾ç­–æ–¹æ³•")
        ]

        for theme, description in priority_themes:
            count = category_counts.get(theme, 0)
            guide += f"\n- **{theme}** ({count}ä»¶)\n  è³ªå•: {description}\n"

        guide += """

---

## ğŸ’¡ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

### ä»Šã™ãã§ãã‚‹ã“ã¨ï¼ˆã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œå¾Œï¼‰
1. `ocr_analysis_prepared_{timestamp}.json` ã§è©³ç´°ãªåˆ†æçµæœã‚’ç¢ºèª
2. å„ãƒ†ãƒ¼ãƒå€™è£œã®ãƒšãƒ¼ã‚¸ã‚’ç¢ºèª
3. è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆã§å®Ÿéš›ã«è¨˜è¿°ã‚’ç¢ºèª

### ãã®æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1. å„ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ç²’åº¦ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿæ–½
2. ãƒ†ãƒ¼ãƒã”ã¨ã«å…·ä½“çš„ãªã‚·ãƒŠãƒªã‚ªã‚’è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
3. 12ãƒ‘ã‚¿ãƒ¼ãƒ³å±•é–‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦å•é¡Œæ¡ˆä½œæˆ
4. æ ¹æ‹ æ–‡çŒ®ã¨ã¨ã‚‚ã«è¨˜éŒ²

### ãƒ„ãƒ¼ãƒ«ãƒ»ãƒªã‚½ãƒ¼ã‚¹
- **è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆ**: `/mnt/c/Users/planj/Downloads/{â‘ ,â‘¡,â‘¢}.pdf`
- **ç¾åœ¨ã®OCR**: `/home/planj/patshinko-exam-app/data/ocr_results_corrected.json`
- **ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©**: `/home/planj/patshinko-exam-app/backend/CORRECTED_12PATTERNS.md`
- **å®Ÿè£…ä¾‹**: `/home/planj/patshinko-exam-app/backend/THEME_PICKUP_IMPLEMENTATION_EXAMPLE.md`

---

**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: âœ… åˆ†æå®Œäº†ãƒ»æº–å‚™å®Œäº†

"""
        return guide

    def run(self):
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        logger.info("=" * 70)
        logger.info("è¬›ç¿’ãƒ†ã‚­ã‚¹ãƒˆ OCRåˆ†æãƒ»æº–å‚™å‡¦ç†é–‹å§‹")
        logger.info("=" * 70)

        # ã‚¹ãƒ†ãƒƒãƒ—1: OCRãƒ­ãƒ¼ãƒ‰
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘OCRçµæœã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        ocr_data = self.load_ocr()
        if not ocr_data:
            logger.error("âŒ OCRãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—")
            return False

        # ã‚¹ãƒ†ãƒƒãƒ—2: å“è³ªåˆ†æ
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘OCRå“è³ªåˆ†æä¸­...")
        analysis = self.analyze_ocr_quality(ocr_data)

        # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘ãƒ†ãƒ¼ãƒã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡ºä¸­...")
        sections = self.extract_key_sections(ocr_data)

        # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¬ã‚¤ãƒ‰ç”Ÿæˆ
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘æº–å‚™ã‚¬ã‚¤ãƒ‰ç”Ÿæˆä¸­...")
        guide = self.generate_preparation_guide(analysis, sections)

        # ã‚¹ãƒ†ãƒƒãƒ—5: çµæœä¿å­˜
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—5ã€‘çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ä¸­...")

        # åˆ†æçµæœJSON
        analysis_path = self.output_dir / f"ocr_analysis_detailed_{self.timestamp}.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… åˆ†æè©³ç´°: {analysis_path}")

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡ºçµæœ
        sections_path = self.output_dir / f"ocr_theme_candidates_{self.timestamp}.json"
        with open(sections_path, 'w', encoding='utf-8') as f:
            json.dump(sections, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… ãƒ†ãƒ¼ãƒå€™è£œ: {sections_path}")

        # ã‚¬ã‚¤ãƒ‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        guide_path = self.output_dir / f"ocr_preparation_guide_{self.timestamp}.md"
        with open(guide_path, 'w', encoding='utf-8') as f:
            f.write(guide)
        logger.info(f"âœ… æº–å‚™ã‚¬ã‚¤ãƒ‰: {guide_path}")

        logger.info("\n" + "=" * 70)
        logger.info("âœ… OCRåˆ†æãƒ»æº–å‚™å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        logger.info("=" * 70)
        logger.info(f"\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        logger.info(f"  1. {analysis_path}")
        logger.info(f"  2. {sections_path}")
        logger.info(f"  3. {guide_path}")
        logger.info(f"\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã‚¬ã‚¤ãƒ‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ã€")
        logger.info(f"ãƒ†ãƒ¼ãƒæŠ½å‡ºã¨12ãƒ‘ã‚¿ãƒ¼ãƒ³å•é¡Œå±•é–‹ã‚’é€²ã‚ã¦ãã ã•ã„")

        return True


def main():
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    processor = LectureOCRAnalyzer()
    success = processor.run()
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
