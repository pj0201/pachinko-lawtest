#!/usr/bin/env python3
"""
Task 5.1: æŠ€è¡“ç®¡ç†åˆ†é‡ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å‹å¼æ¤œå®šé–¢é€£ + éŠæŠ€æ©Ÿç®¡ç†ãƒ†ãƒ¼ãƒã‹ã‚‰ã€
å•é¡Œç”Ÿæˆç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ã™ã‚‹
"""

import json
import os
from pathlib import Path
from collections import defaultdict

print("=" * 80)
print("ã€Task 5.1: æŠ€è¡“ç®¡ç†åˆ†é‡ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€‘")
print("=" * 80)

# 1. è¬›ç¿’ãƒ†ãƒ¼ãƒã®ç¢ºèª
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—1: è¬›ç¿’ãƒ†ãƒ¼ãƒã®ç¢ºèª")

lecture_dir = Path("rag_data/lecture_text")
if lecture_dir.exists():
    lecture_files = sorted(lecture_dir.glob("*.txt"))
    print(f"  è¦‹ã¤ã‘ãŸãƒ†ãƒ¼ãƒ: {len(lecture_files)}å€‹")
else:
    print(f"âŒ {lecture_dir} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    lecture_files = []

# 2. æŠ€è¡“ç®¡ç†åˆ†é‡ãƒ†ãƒ¼ãƒã®åˆ†é¡
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—2: æŠ€è¡“ç®¡ç†åˆ†é‡ãƒ†ãƒ¼ãƒã‚’åˆ†é¡")

# æŠ€è¡“ç®¡ç†åˆ†é‡ã«è©²å½“ã™ã‚‹ãƒ†ãƒ¼ãƒã‚’å®šç¾©ï¼ˆå®Ÿåœ¨ã™ã‚‹ãƒ†ãƒ¼ãƒã®ã¿ï¼‰
# Week 4ã§æœªä½¿ç”¨ã®å‹å¼æ¤œå®šé–¢é€£ + éŠæŠ€æ©Ÿç®¡ç†ãƒ†ãƒ¼ãƒ
technology_themes = {
    "type_certification": [
        "theme_025_å‹å¼æ¤œå®šã®ç”³è«‹æ–¹æ³•.txt",
        "theme_026_å‹å¼æ¤œå®šæ›´æ–°ç”³è«‹ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°.txt",
    ],
    "gaming_machine_management": [
        "theme_027_åŸºæ¿ã‚±ãƒ¼ã‚¹ã®ã‹ã—ã‚ã¨ç®¡ç†.txt",
        "theme_028_å¤–éƒ¨ç«¯å­æ¿ã®ç®¡ç†.txt",
        "theme_029_æ•…éšœéŠæŠ€æ©Ÿã®å¯¾å¿œ.txt",
        "theme_031_æ—§æ©Ÿæ¢°ã®å›åã¨å»ƒæ£„.txt",
        "theme_037_éŠæŠ€æ©Ÿã®ä¿å®ˆç®¡ç†.txt",
        "theme_038_éŠæŠ€æ©Ÿã®ç‚¹æ¤œãƒ»ä¿å®ˆè¨ˆç”».txt",
    ]
}

tech_theme_count = sum(len(v) for v in technology_themes.values())
print(f"""
  æŠ€è¡“ç®¡ç†åˆ†é‡ãƒ†ãƒ¼ãƒ: {tech_theme_count}å€‹
  ã‚«ãƒ†ã‚´ãƒª:
    - å‹å¼æ¤œå®šé–¢é€£: {len(technology_themes['type_certification'])}å€‹
    - éŠæŠ€æ©Ÿç®¡ç†: {len(technology_themes['gaming_machine_management'])}å€‹
""")

# 3. ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ã®å®šç¾©
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—3: ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ã‚’å®šç¾©")

chunking_strategy = {
    "method": "logical_units",
    "target_tokens": 500,
    "delimiter": ["ã€‚\n", "ã€‚", "\n\n"],
    "min_chunk_size": 50,
    "max_chunk_size": 1000,
    "categories": list(technology_themes.keys())
}

print(f"""
  ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æ–¹å¼: {chunking_strategy['method']}
  ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {chunking_strategy['target_tokens']}
  æœ€å°/æœ€å¤§: {chunking_strategy['min_chunk_size']}/{chunking_strategy['max_chunk_size']}
""")

# 4. ãƒ†ãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—4: æŠ€è¡“ç®¡ç†åˆ†é‡ãƒ†ãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€")

technology_chunks = []
category_stats = defaultdict(lambda: {"count": 0, "tokens": 0})

for category, theme_files in technology_themes.items():
    print(f"\n  ã€{category}ã€‘")

    for theme_file in theme_files:
        theme_path = lecture_dir / theme_file

        if not theme_path.exists():
            print(f"    âš ï¸  {theme_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            continue

        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        try:
            with open(theme_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()

            if not content:
                print(f"    âš ï¸  {theme_file} ãŒç©ºã§ã™")
                continue

            # ç°¡æ˜“çš„ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¨å®š
            token_count = len(content) // 3

            # ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
            chunk = {
                "chunk_id": f"technology_{category}_{len(technology_chunks):03d}",
                "category": category,
                "source_file": theme_file,
                "content": content,
                "token_count": token_count,
                "source": "lecture_materials"
            }

            technology_chunks.append(chunk)
            category_stats[category]["count"] += 1
            category_stats[category]["tokens"] += token_count

            print(f"    âœ“ {theme_file:45} ({token_count:5}ãƒˆãƒ¼ã‚¯ãƒ³)")

        except Exception as e:
            print(f"    âœ— ã‚¨ãƒ©ãƒ¼: {theme_file} - {e}")

# 5. çµ±è¨ˆæƒ…å ±
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—5: çµ±è¨ˆé›†è¨ˆ")

total_chunks = len(technology_chunks)
total_tokens = sum(s["tokens"] for s in category_stats.values())

print(f"""
  ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {total_chunks}å€‹
  ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_tokens}ãƒˆãƒ¼ã‚¯ãƒ³

  ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆã€‘
""")

for category, stats in category_stats.items():
    tokens = stats["tokens"]
    percentage = (tokens / total_tokens * 100) if total_tokens > 0 else 0
    print(f"    {category:30} {stats['count']:2}å€‹ ({tokens:5}ãƒˆãƒ¼ã‚¯ãƒ³, {percentage:5.1f}%)")

# 6. å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—6: å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©")

output_schema = {
    "metadata": {
        "task": "Task 5.1 - æŠ€è¡“ç®¡ç†åˆ†é‡ãƒ‡ãƒ¼ã‚¿æº–å‚™",
        "domain": "technology",
        "total_chunks": total_chunks,
        "total_tokens": total_tokens,
        "chunking_method": "logical_units",
        "source": "lecture_materials"
    },
    "chunking_strategy": chunking_strategy,
    "category_distribution": dict(category_stats),
    "sample_chunks": technology_chunks[:3]
}

print(f"""
  å‡ºåŠ›å½¢å¼: JSONL (1è¡Œ1ãƒãƒ£ãƒ³ã‚¯) + ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSON

  å„ãƒãƒ£ãƒ³ã‚¯ã®æ§‹é€ :
  {{
    "chunk_id": "unique_id",
    "category": "category_name",
    "source_file": "theme_XXX_...txt",
    "content": "...",
    "token_count": 500,
    "source": "lecture_materials"
  }}
""")

# 7. æŠ€è¡“ç®¡ç†åˆ†é‡ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—7: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")

output_dir = Path("data")
output_dir.mkdir(exist_ok=True)

# JSONLå½¢å¼ã§ä¿å­˜
jsonl_path = output_dir / "technology_domain_chunks_prepared.jsonl"
with open(jsonl_path, 'w', encoding='utf-8') as f:
    for chunk in technology_chunks:
        f.write(json.dumps(chunk, ensure_ascii=False) + "\n")

print(f"  âœ“ JSONLä¿å­˜: {jsonl_path} ({total_chunks}è¡Œ)")

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã‚’ä¿å­˜
metadata_path = output_dir / "technology_domain_chunks_metadata.json"
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(output_schema, f, indent=2, ensure_ascii=False)

print(f"  âœ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {metadata_path}")

# 8. ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—8: ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒ³ã‚¯è¡¨ç¤º")

if technology_chunks:
    for i, chunk in enumerate(technology_chunks[:2], 1):
        print(f"\n  ã€ã‚µãƒ³ãƒ—ãƒ« {i}: {chunk['chunk_id']}ã€‘")
        print(f"    ã‚«ãƒ†ã‚´ãƒª: {chunk['category']}")
        print(f"    ã‚½ãƒ¼ã‚¹: {chunk['source_file']}")
        print(f"    ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {chunk['token_count']}")
        preview = chunk['content'][:100].replace('\n', ' ')
        print(f"    å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {preview}...")

# 9. å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
print("\n" + "=" * 80)
print("ã€Task 5.1 å®Œäº† - æŠ€è¡“ç®¡ç†åˆ†é‡ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†ã€‘")
print("=" * 80)

print(f"""
âœ… æº–å‚™å®Œäº†ï¼š
  - ãƒãƒ£ãƒ³ã‚¯æ•°: {total_chunks}å€‹
  - ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_tokens}ãƒˆãƒ¼ã‚¯ãƒ³
  - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:
    - {jsonl_path}
    - {metadata_path}

ğŸ“Š æŠ€è¡“ç®¡ç†åˆ†é‡ã®å†…å®¹ï¼š
  - å‹å¼æ¤œå®šé–¢é€£ï¼ˆç”³è«‹æ–¹æ³•ã€ä¸åˆæ ¼æ™‚å¯¾å¿œï¼‰
  - éŠæŠ€æ©Ÿç®¡ç†ï¼ˆä¿å®ˆã€æ–°å°å°å…¥ã€æ•…éšœå¯¾å¿œã€åŸºæ¿ç®¡ç†ç­‰ï¼‰
  - æ™¯å“è¦åˆ¶åŸºæº–ï¼ˆç¨®é¡åˆ¶é™ç­‰ï¼‰

ğŸš€ æ¬¡ã‚¿ã‚¹ã‚¯ï¼ˆTask 5.2ï¼‰ï¼š
  - Claude APIã‚’ä½¿ç”¨ã—ã¦50å•ç”Ÿæˆ
  - è¤‡åˆèªå¯¾å¿œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
  - `output/technology_domain_50_raw.json` ã«å‡ºåŠ›
""")

print("=" * 80)
