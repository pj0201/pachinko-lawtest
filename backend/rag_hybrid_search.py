#!/usr/bin/env python3
"""
RAG ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
BM25ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰+ å¯†ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼‰ã‚’çµ±åˆ

æ¤œç´¢æˆ¦ç•¥:
1. BM25ã‚¹ã‚³ã‚¢: æ­£ç¢ºãªæ¡æ–‡æ¤œç´¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒï¼‰
2. å¯†ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ã‚³ã‚¢: æ„å‘³çš„é–¢é€£æ€§ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼‰
3. ãƒã‚¤ãƒ–ãƒªã‚¹ã‚¹ã‚³ã‚¢: ä¸¡è€…ã®é‡ã¿ä»˜ã‘çµ±åˆ

å‡¦ç†ãƒ•ãƒ­ãƒ¼:
1. é¢¨å–¶æ³•ãƒ‡ãƒ¼ã‚¿INDEXåŒ–ï¼ˆæ¡æ–‡å˜ä½ï¼‰
2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ä¸ï¼ˆæ¡ç•ªå·ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚«ãƒ†ã‚´ãƒªï¼‰
3. BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
4. OpenAI Embeddingsç”Ÿæˆ
5. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè¡Œ
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - RAG_HYBRID - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===== ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ =====
@dataclass
class LegalClause:
    """æ³•ä»¤æ¡æ–‡"""
    article_number: str        # æ¡ç•ªå· (e.g., "1æ¡", "1æ¡ã®2")
    title: str                 # æ¡æ–‡ã‚¿ã‚¤ãƒˆãƒ«
    category: str              # ã‚«ãƒ†ã‚´ãƒª (e.g., "å–¶æ¥­è¨±å¯", "å–¶æ¥­ç¦æ­¢")
    content: str               # æ¡æ–‡æœ¬æ–‡
    subclauses: List[str]      # é …ï¼ˆè¤‡æ•°ï¼‰
    related_articles: List[str] # é–¢é€£æ¡æ–‡
    chunk_id: str              # ãƒãƒ£ãƒ³ã‚¯IDï¼ˆæ¤œç´¢ã‚­ãƒ¼ï¼‰
    embedding: Optional[List[float]] = None  # ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿

class RAGHybridSearch:
    """RAG ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        self.repo_root = Path("/home/planj/patshinko-exam-app")
        self.rag_data_dir = self.repo_root / "rag_data"
        self.legal_ref_dir = self.rag_data_dir / "legal_references"
        self.rag_data_dir.mkdir(parents=True, exist_ok=True)

        self.legal_clauses: List[LegalClause] = []
        self.bm25_index: Dict = {}
        self.metadata_index: Dict = {}

        logger.info("âœ… RAG ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–")

    def create_clause_index(self, legal_data: Dict) -> List[LegalClause]:
        """é¢¨å–¶æ³•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¡æ–‡INDEXã‚’ä½œæˆ"""
        clauses = []

        # é¢¨å–¶æ³•ã®æ§‹é€ ã«åŸºã¥ã„ã¦æ¡æ–‡ã‚’æŠ½å‡º
        for article_num, article_info in legal_data.items():
            if not isinstance(article_info, dict):
                continue

            clause = LegalClause(
                article_number=article_num,
                title=article_info.get("title", ""),
                category=article_info.get("category", ""),
                content=article_info.get("content", ""),
                subclauses=article_info.get("subclauses", []),
                related_articles=article_info.get("related_articles", []),
                chunk_id=f"winei_{article_num}_{datetime.now().timestamp()}"
            )
            clauses.append(clause)

        logger.info(f"ğŸ“Š {len(clauses)}å€‹ã®æ¡æ–‡ã‚’INDEXåŒ–")
        return clauses

    def build_bm25_index(self, clauses: List[LegalClause]) -> Dict:
        """BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆè¤‡åˆèªå¯¾å¿œç‰ˆï¼‰"""
        index = {}

        # è¤‡åˆèªè¾æ›¸ï¼ˆé¢¨å–¶æ³•ã§é »å‡ºã™ã‚‹é‡è¦èªï¼‰
        compound_words = {
            "å–¶æ¥­è¨±å¯", "å–¶æ¥­ç¦æ­¢", "å–¶æ¥­æ‰€", "å–¶æ¥­æ–¹é‡", "å–¶æ¥­æ™‚é–“",
            "å–¶æ¥­æ‰€åŸºæº–", "å–¶æ¥­åœæ­¢", "å–¶æ¥­å»ƒæ­¢",
            "éŠæŠ€æ©Ÿ", "æ™¯å“", "æ™¯å“è¦åˆ¶", "æ™¯å“äº¤æ›",
            "ç”³è«‹", "ç”³è«‹è€…", "è¨±å¯ç”³è«‹",
            "é•å", "é•åè¡Œç‚º", "é•åè€…",
            "æ¤œå®š", "æ¤œå®šæ©Ÿå™¨", "æ©Ÿæ¢°", "æ©Ÿå™¨",
            "ä¸æ­£", "ä¸æ­£åˆ©ç”¨", "ä¸æ­£è¡Œç‚º",
            "æ³•ä»¤", "é¢¨å–¶æ³•", "æ¡æ–‡", "æ¡é …",
            "å–æ¶ˆ", "å–ã‚Šæ¶ˆã—", "å»ƒæ­¢", "å¤‰æ›´",
            "ç›£ç£", "ç›£ç£å®˜åº", "æŒ‡å°", "æŒ‡ç¤º",
            "è¨˜éŒ²", "å ±å‘Š", "æå‡º", "ç¢ºèª"
        }

        for clause in clauses:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆè¤‡åˆèªå¯¾å¿œï¼‰
            text = f"{clause.title} {clause.content}"
            words = set()

            # 1. è¤‡åˆèªè¾æ›¸ã‹ã‚‰ã®ãƒãƒƒãƒãƒ³ã‚°
            for compound in compound_words:
                if compound in text:
                    words.add(compound)

            # 2. N-gramå‡¦ç†ï¼ˆ3æ–‡å­—ã€4æ–‡å­—ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
            # ã‚¹ãƒšãƒ¼ã‚¹ãƒ»å¥èª­ç‚¹ã‚’é™¤å»ã—ã¦ã‹ã‚‰å‡¦ç†
            clean_text = text.replace(" ", "").replace("\n", "")
            clean_text = clean_text.translate(str.maketrans('', '', 'ã€‚ã€ï¼ˆï¼‰ã€Œã€ã€ã€\t'))

            # 3æ–‡å­—N-gram
            for i in range(len(clean_text) - 2):
                trigram = clean_text[i:i+3]
                if len(trigram) == 3 and all(ord(c) >= 0x4E00 for c in trigram):
                    words.add(trigram)

            # 4æ–‡å­—N-gram
            for i in range(len(clean_text) - 3):
                fourgram = clean_text[i:i+4]
                if len(fourgram) == 4 and all(ord(c) >= 0x4E00 for c in fourgram):
                    words.add(fourgram)

            # 3. å˜ä¸€ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
            important_keywords = ["å–¶æ¥­", "è¨±å¯", "é•å", "å–æ¶ˆ", "æ¤œå®š", "æ©Ÿ", "æ™¯å“",
                                 "ç¦æ­¢", "åœæ­¢", "å»ƒæ­¢", "ç”³è«‹", "ä¸æ­£", "ç›£ç£",
                                 "æ³•ä»¤", "æŒ‡ç¤º", "è¨˜éŒ²", "å ±å‘Š", "æå‡º"]
            for keyword in important_keywords:
                if keyword in text:
                    words.add(keyword)

            # 4. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ç™»éŒ²
            for keyword in words:
                if keyword not in index:
                    index[keyword] = []
                if clause.chunk_id not in index[keyword]:  # é‡è¤‡æ’é™¤
                    index[keyword].append(clause.chunk_id)

        logger.info(f"âœ… BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {len(index)}å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¤‡åˆèªå¯¾å¿œï¼‰")
        return index

    def build_metadata_index(self, clauses: List[LegalClause]) -> Dict:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿INDEXã‚’æ§‹ç¯‰"""
        index = {
            "by_chunk_id": {},
            "by_category": {},
            "by_article": {}
        }

        for clause in clauses:
            # chunk_id ã§ã®ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆæ¤œç´¢
            index["by_chunk_id"][clause.chunk_id] = asdict(clause)

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥INDEX
            if clause.category not in index["by_category"]:
                index["by_category"][clause.category] = []
            index["by_category"][clause.category].append(clause.chunk_id)

            # æ¡ç•ªå·åˆ¥INDEX
            index["by_article"][clause.article_number] = clause.chunk_id

        logger.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿INDEX: {len(index['by_chunk_id'])}ä»¶")
        return index

    def search_bm25(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """BM25æ¤œç´¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒï¼‰- è¤‡åˆèªå¯¾å¿œç‰ˆ"""

        # ã‚¯ã‚¨ãƒªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        query_keywords = set()

        # è¤‡åˆèªè¾æ›¸ï¼ˆæ¤œç´¢ç”¨ï¼‰
        compound_words = {
            "å–¶æ¥­è¨±å¯", "å–¶æ¥­ç¦æ­¢", "å–¶æ¥­æ‰€", "å–¶æ¥­æ–¹é‡", "å–¶æ¥­æ™‚é–“",
            "å–¶æ¥­æ‰€åŸºæº–", "å–¶æ¥­åœæ­¢", "å–¶æ¥­å»ƒæ­¢",
            "éŠæŠ€æ©Ÿ", "æ™¯å“", "æ™¯å“è¦åˆ¶", "æ™¯å“äº¤æ›",
            "ç”³è«‹", "ç”³è«‹è€…", "è¨±å¯ç”³è«‹",
            "é•å", "é•åè¡Œç‚º", "é•åè€…",
            "æ¤œå®š", "æ¤œå®šæ©Ÿå™¨", "æ©Ÿæ¢°", "æ©Ÿå™¨",
            "ä¸æ­£", "ä¸æ­£åˆ©ç”¨", "ä¸æ­£è¡Œç‚º",
            "æ³•ä»¤", "é¢¨å–¶æ³•", "æ¡æ–‡", "æ¡é …",
            "å–æ¶ˆ", "å–ã‚Šæ¶ˆã—", "å»ƒæ­¢", "å¤‰æ›´",
            "ç›£ç£", "ç›£ç£å®˜åº", "æŒ‡å°", "æŒ‡ç¤º",
            "è¨˜éŒ²", "å ±å‘Š", "æå‡º", "ç¢ºèª"
        }

        # 1. è¤‡åˆèªãƒãƒƒãƒãƒ³ã‚°
        for compound in compound_words:
            if compound in query:
                query_keywords.add(compound)

        # 2. å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆã‚¹ãƒšãƒ¼ã‚¹åˆ†å‰²ï¼‰
        for word in query.split():
            word = word.strip('ã€‚ã€ï¼ˆï¼‰ã€Œã€ã€ã€\n\t ')
            if len(word) >= 2:
                query_keywords.add(word)

        # 3. N-gramæŠ½å‡ºï¼ˆ3æ–‡å­—ã€4æ–‡å­—ï¼‰
        clean_query = query.replace(" ", "").replace("\n", "")
        clean_query = clean_query.translate(str.maketrans('', '', 'ã€‚ã€ï¼ˆï¼‰ã€Œã€ã€ã€\t'))

        for i in range(len(clean_query) - 2):
            trigram = clean_query[i:i+3]
            if len(trigram) == 3 and all(ord(c) >= 0x4E00 for c in trigram):
                query_keywords.add(trigram)

        for i in range(len(clean_query) - 3):
            fourgram = clean_query[i:i+4]
            if len(fourgram) == 4 and all(ord(c) >= 0x4E00 for c in fourgram):
                query_keywords.add(fourgram)

        # 4. BM25ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = {}
        for keyword in query_keywords:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
            for chunk_id in self.bm25_index.get(keyword, []):
                if chunk_id not in scores:
                    scores[chunk_id] = 0
                scores[chunk_id] += 1  # BM25ã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“ç‰ˆï¼‰

        # ã‚¹ã‚³ã‚¢ã§é™é †ã‚½ãƒ¼ãƒˆ
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        return ranked

    def hybrid_search(self, query: str, top_k: int = 5,
                     bm25_weight: float = 0.4, semantic_weight: float = 0.6) -> List[Dict]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢: BM25 + ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢"""

        # 1. BM25æ¤œç´¢
        bm25_results = self.search_bm25(query, top_k * 2)
        logger.info(f"ğŸ“Š BM25æ¤œç´¢: {len(bm25_results)}ä»¶")

        # 2. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢è¨ˆç®—
        hybrid_scores = {}

        for chunk_id, bm25_score in bm25_results:
            clause = self.metadata_index["by_chunk_id"].get(chunk_id)
            if clause:
                # ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
                normalized_bm25 = min(bm25_score / (max([s[1] for s in bm25_results]) or 1), 1.0)
                # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚¹ã‚³ã‚¢ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ã¯çœç•¥ï¼‰
                semantic_score = 0.5  # ä»®ã‚¹ã‚³ã‚¢

                # é‡ã¿ä»˜ã‘çµ±åˆ
                hybrid_score = (normalized_bm25 * bm25_weight +
                              semantic_score * semantic_weight)

                hybrid_scores[chunk_id] = {
                    "clause": clause,
                    "bm25_score": bm25_score,
                    "semantic_score": semantic_score,
                    "hybrid_score": hybrid_score
                }

        # 3. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        ranked_results = sorted(
            hybrid_scores.items(),
            key=lambda x: x[1]["hybrid_score"],
            reverse=True
        )[:top_k]

        return [
            {
                "chunk_id": chunk_id,
                "clause": result["clause"],
                "scores": {
                    "bm25": result["bm25_score"],
                    "semantic": result["semantic_score"],
                    "hybrid": result["hybrid_score"]
                }
            }
            for chunk_id, result in ranked_results
        ]

    def save_index(self):
        """INDEXã‚’JSONã§ä¿å­˜"""
        index_data = {
            "timestamp": datetime.now().isoformat(),
            "total_clauses": len(self.legal_clauses),
            "metadata_index": self.metadata_index,
            "bm25_keywords_count": len(self.bm25_index)
        }

        output_file = self.rag_data_dir / "hybrid_index.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… INDEXä¿å­˜: {output_file}")

    def load_legal_references_from_files(self) -> Dict:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ³•ä»¤ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
        legal_data = {}

        if not self.legal_ref_dir.exists():
            logger.warning(f"âŒ æ³•ä»¤ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã—: {self.legal_ref_dir}")
            return legal_data

        logger.info(f"ğŸ“š æ³•ä»¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        for file_path in sorted(self.legal_ref_dir.glob("*.txt")):
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    article_range = file_path.stem  # e.g., "é¢¨å–¶æ³•_ç¬¬1ã€œ10æ¡"
                    legal_data[article_range] = {
                        "title": article_range,
                        "category": "æ³•ä»¤",
                        "content": content[:2000],  # æœ€åˆã®2000æ–‡å­—
                        "subclauses": [],
                        "related_articles": []
                    }
                    logger.info(f"   âœ… {file_path.name} ({len(content)} æ–‡å­—)")
            except Exception as e:
                logger.warning(f"   âš ï¸  {file_path.name}: {str(e)[:50]}")

        logger.info(f"âœ… {len(legal_data)}ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†\n")
        return legal_data

    def initialize(self, legal_data: Dict = None):
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–"""
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ³•ä»¤ã‚’èª­ã¿è¾¼ã‚€å ´åˆ
        if legal_data is None:
            legal_data = self.load_legal_references_from_files()

        # 1. æ¡æ–‡INDEXåŒ–
        self.legal_clauses = self.create_clause_index(legal_data)

        # 2. BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        self.bm25_index = self.build_bm25_index(self.legal_clauses)

        # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿INDEXæ§‹ç¯‰
        self.metadata_index = self.build_metadata_index(self.legal_clauses)

        # 4. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜
        self.save_index()

        logger.info("âœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    logger.info("ğŸš€ RAG ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ ãƒ†ã‚¹ãƒˆé–‹å§‹")

    # ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•èª­ã¿è¾¼ã¿ï¼‰
    engine = RAGHybridSearch()
    engine.initialize()

    # ãƒ†ã‚¹ãƒˆæ¤œç´¢
    test_queries = [
        "é¢¨ä¿—å–¶æ¥­ã®å®šç¾©",
        "å–¶æ¥­æ–¹æ³•ã®è¦åˆ¶",
        "åœ°åŸŸã®å¥å…¨æ€§"
    ]

    logger.info("\n" + "="*70)
    logger.info("ğŸ“Š ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ãƒ†ã‚¹ãƒˆçµæœ")
    logger.info("="*70)

    test_queries = [
        "å–¶æ¥­è¨±å¯",
        "é•å",
        "å–æ¶ˆ",
        "æ¤œå®šæ©Ÿ",
        "æ™¯å“"
    ]

    for query in test_queries:
        logger.info(f"\nã€ã‚¯ã‚¨ãƒªã€‘{query}")
        results = engine.hybrid_search(query, top_k=3)

        if results:
            for i, result in enumerate(results, 1):
                logger.info(f"  {i}. {result['clause']['title']}")
                logger.info(f"     BM25: {result['scores']['bm25']:.2f} | Semantic: {result['scores']['semantic']:.2f} | Hybrid: {result['scores']['hybrid']:.3f}")
        else:
            logger.info("  â†’ è©²å½“ãªã—")

if __name__ == "__main__":
    main()
