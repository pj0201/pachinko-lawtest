#!/usr/bin/env python3
"""
è©¦é¨“å•é¡Œã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç›®çš„ï¼š
- è¬›ç¾©è³‡æ–™ï¼ˆâ‘ â‘¡â‘¢.pdfï¼‰ã®OCRçµæœ
- é¢¨å–¶æ³•ãƒ»é¢¨å–¶æ³•æ–½è¡Œè¦å‰‡
- æ—¢å­˜ã®è©¦é¨“å•é¡Œï¼ˆ638å• or 230å•ï¼‰
ã‚’åˆ†æã—ã€ã‚½ãƒ¼ã‚¹è³‡æ–™ã®å†…å®¹ãŒå•é¡Œã§ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
"""

import json
import re
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime
import sys

class CoverageGapAnalyzer:
    """ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        self.base_dir = Path(__file__).parent.parent
        self.problems = []
        self.lecture_chunks = []
        self.ocr_pages = []

        # åˆ†æçµæœ
        self.source_topics = defaultdict(list)
        self.problem_topics = defaultdict(list)
        self.coverage_gaps = []

    def load_problems(self):
        """è©¦é¨“å•é¡Œã‚’èª­ã¿è¾¼ã‚€"""
        print("ğŸ“– è©¦é¨“å•é¡Œã‚’èª­ã¿è¾¼ã¿ä¸­...")

        problems_file = self.base_dir / "data" / "problems.json"
        with open(problems_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.problems = data.get('problems', [])

        print(f"   âœ… å•é¡Œæ•°: {len(self.problems)}å•")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        categories = Counter([p.get('category', 'unknown') for p in self.problems])
        print(f"   ã‚«ãƒ†ã‚´ãƒªåˆ¥:")
        for cat, count in categories.most_common():
            print(f"      - {cat}: {count}å•")

        return len(self.problems)

    def load_lecture_materials(self):
        """è¬›ç¾©è³‡æ–™ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã‚€"""
        print("\nğŸ“š è¬›ç¾©è³‡æ–™ã‚’èª­ã¿è¾¼ã¿ä¸­...")

        chunks_file = self.base_dir / "backend" / "data" / "lecture_materials_chunks.json"
        if not chunks_file.exists():
            print("   âš ï¸ ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return 0

        with open(chunks_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.lecture_chunks = data.get('chunks', [])

        print(f"   âœ… ãƒãƒ£ãƒ³ã‚¯æ•°: {len(self.lecture_chunks)}")

        # PDFåˆ¥çµ±è¨ˆ
        pdf_dist = Counter([c.get('pdf_index') for c in self.lecture_chunks])
        for pdf_idx, count in sorted(pdf_dist.items()):
            print(f"      - PDF{pdf_idx}: {count}ãƒãƒ£ãƒ³ã‚¯")

        return len(self.lecture_chunks)

    def load_ocr_pages(self):
        """OCRãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        print("\nğŸ“„ OCRãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")

        ocr_file = self.base_dir / "data" / "old_problems" / "ocr_results_corrected.json"
        with open(ocr_file, 'r', encoding='utf-8') as f:
            self.ocr_pages = json.load(f)

        print(f"   âœ… ãƒšãƒ¼ã‚¸æ•°: {len(self.ocr_pages)}")
        return len(self.ocr_pages)

    def extract_keywords_from_text(self, text, min_length=3):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns = [
            r'ç¬¬\d+æ¡(?:ã®\d+)?',  # æ¡æ–‡ç•ªå·
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡',  # æ¼¢æ•°å­—ã®æ¡æ–‡
            r'éŠæŠ€æ©Ÿå–æ‰±ä¸»ä»»è€…',
            r'é¢¨å–¶æ³•',
            r'é¢¨ä¿—å–¶æ¥­',
            r'å‹å¼æ¤œå®š',
            r'èªå®š',
            r'æ¤œæŸ»',
            r'ä¿å®ˆç®¡ç†',
            r'ä¸æ­£[æ”¹é€ |ä½¿ç”¨|è¡Œç‚º]',
            r'å–¶æ¥­[è¨±å¯|åœæ­¢|æ™‚é–“]',
            r'ç½°å‰‡',
            r'æ™¯å“',
            r'å°„å¹¸å¿ƒ',
            r'ç™»éŒ²åˆ¶åº¦',
            r'è²©å£²æ¥­è€…',
            r'è£½é€ æ¥­è€…',
            r'ä¸­å¤éŠæŠ€æ©Ÿ',
            r'æ–°è¦[è©¦é¨“|è¬›ç¿’]',
            r'æ›´æ–°[è©¦é¨“|è¬›ç¿’]',
            r'æœ‰åŠ¹æœŸé–“',
            r'ä¿è¨¼æ›¸',
            r'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£',
            r'ãƒãƒƒãƒ—',
            r'åŸºæ¿',
            r'ãƒªã‚µã‚¤ã‚¯ãƒ«',
            r'å»ƒæ£„ç‰©',
        ]

        keywords = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            keywords.extend(matches)

        # è¿½åŠ ï¼šåè©å¥æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        # ã‚«ã‚¿ã‚«ãƒŠèª
        katakana_words = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]{3,}', text)
        keywords.extend(katakana_words)

        return list(set(keywords))

    def analyze_lecture_topics(self):
        """è¬›ç¾©è³‡æ–™ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†æ"""
        print("\nğŸ” è¬›ç¾©è³‡æ–™ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†æä¸­...")

        topic_frequency = Counter()
        topic_locations = defaultdict(list)

        for i, chunk in enumerate(self.lecture_chunks):
            text = chunk.get('text', '')
            pdf_idx = chunk.get('pdf_index')
            page_num = chunk.get('page_number')

            keywords = self.extract_keywords_from_text(text)

            for keyword in keywords:
                topic_frequency[keyword] += 1
                topic_locations[keyword].append({
                    'pdf': pdf_idx,
                    'page': page_num,
                    'chunk_index': i
                })

        # ãƒˆãƒƒãƒ—50ãƒˆãƒ”ãƒƒã‚¯ã‚’ä¿å­˜
        self.source_topics = dict(topic_frequency.most_common(50))

        print(f"   âœ… æ¤œå‡ºã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯: {len(topic_frequency)}")
        print(f"\n   ãƒˆãƒƒãƒ—10ãƒˆãƒ”ãƒƒã‚¯:")
        for topic, count in topic_frequency.most_common(10):
            print(f"      - {topic}: {count}å›")

        return topic_frequency, topic_locations

    def analyze_problem_topics(self):
        """è©¦é¨“å•é¡Œã®ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†æ"""
        print("\nğŸ” è©¦é¨“å•é¡Œã®ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†æä¸­...")

        topic_frequency = Counter()

        for problem in self.problems:
            question = problem.get('question', '')
            options = ' '.join([opt.get('text', '') for opt in problem.get('options', [])])
            explanation = problem.get('explanation', '')

            full_text = f"{question} {options} {explanation}"
            keywords = self.extract_keywords_from_text(full_text)

            for keyword in keywords:
                topic_frequency[keyword] += 1

        self.problem_topics = dict(topic_frequency.most_common(50))

        print(f"   âœ… å•é¡Œã«å«ã¾ã‚Œã‚‹ãƒˆãƒ”ãƒƒã‚¯: {len(topic_frequency)}")
        print(f"\n   ãƒˆãƒƒãƒ—10ãƒˆãƒ”ãƒƒã‚¯:")
        for topic, count in topic_frequency.most_common(10):
            print(f"      - {topic}: {count}å›")

        return topic_frequency

    def identify_coverage_gaps(self, lecture_topics, problem_topics):
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—ã‚’ç‰¹å®š"""
        print("\nğŸ” ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—ã‚’ç‰¹å®šä¸­...")

        gaps = []

        # ã‚½ãƒ¼ã‚¹ã«ã‚ã‚‹ãŒå•é¡Œã«ãªã„ãƒˆãƒ”ãƒƒã‚¯
        source_only = set(lecture_topics.keys()) - set(problem_topics.keys())

        # é »åº¦ãŒé«˜ã„ã®ã«å•é¡ŒãŒå°‘ãªã„ãƒˆãƒ”ãƒƒã‚¯
        for topic, source_count in lecture_topics.items():
            problem_count = problem_topics.get(topic, 0)

            # ã‚½ãƒ¼ã‚¹ã§10å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹ãŒã€å•é¡Œã§ã¯3å›ä»¥ä¸‹
            if source_count >= 10 and problem_count <= 3:
                gaps.append({
                    'topic': topic,
                    'source_frequency': source_count,
                    'problem_frequency': problem_count,
                    'gap_ratio': source_count / max(problem_count, 1),
                    'status': 'under_represented'
                })

        # ã‚½ãƒ¼ã‚¹ã®ã¿ã«å­˜åœ¨ï¼ˆå•é¡Œã§å…¨ãæ‰±ã‚ã‚Œã¦ã„ãªã„ï¼‰
        for topic in source_only:
            if lecture_topics[topic] >= 5:  # 5å›ä»¥ä¸Šå‡ºç¾
                gaps.append({
                    'topic': topic,
                    'source_frequency': lecture_topics[topic],
                    'problem_frequency': 0,
                    'gap_ratio': float('inf'),
                    'status': 'not_covered'
                })

        # ã‚®ãƒ£ãƒƒãƒ—ç‡ã§ã‚½ãƒ¼ãƒˆ
        gaps.sort(key=lambda x: x['gap_ratio'] if x['gap_ratio'] != float('inf') else 999999, reverse=True)

        self.coverage_gaps = gaps

        print(f"   âœ… æ¤œå‡ºã•ã‚ŒãŸã‚®ãƒ£ãƒƒãƒ—: {len(gaps)}ãƒˆãƒ”ãƒƒã‚¯")

        return gaps

    def generate_report(self):
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\nğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

        report = {
            'metadata': {
                'analysis_date': datetime.now().isoformat(),
                'total_problems': len(self.problems),
                'total_lecture_chunks': len(self.lecture_chunks),
                'total_ocr_pages': len(self.ocr_pages)
            },
            'statistics': {
                'source_topics_count': len(self.source_topics),
                'problem_topics_count': len(self.problem_topics),
                'coverage_gaps_count': len(self.coverage_gaps)
            },
            'top_source_topics': dict(list(self.source_topics.items())[:20]),
            'top_problem_topics': dict(list(self.problem_topics.items())[:20]),
            'coverage_gaps': self.coverage_gaps[:30],  # ãƒˆãƒƒãƒ—30ã‚®ãƒ£ãƒƒãƒ—
            'recommendations': self._generate_recommendations()
        }

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        output_file = self.base_dir / "backend" / "data" / "coverage_gap_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"   âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_file}")

        return report

    def _generate_recommendations(self):
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        # æœªã‚«ãƒãƒ¼ãƒˆãƒ”ãƒƒã‚¯
        not_covered = [g for g in self.coverage_gaps if g['status'] == 'not_covered']
        if not_covered:
            recommendations.append({
                'priority': 'high',
                'category': 'æœªã‚«ãƒãƒ¼ãƒˆãƒ”ãƒƒã‚¯',
                'count': len(not_covered),
                'description': f'{len(not_covered)}å€‹ã®ãƒˆãƒ”ãƒƒã‚¯ãŒã‚½ãƒ¼ã‚¹è³‡æ–™ã«å­˜åœ¨ã™ã‚‹ãŒã€è©¦é¨“å•é¡Œã§å…¨ãæ‰±ã‚ã‚Œã¦ã„ã¾ã›ã‚“ã€‚',
                'action': 'ã“ã‚Œã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹å•é¡Œã‚’æ–°è¦ä½œæˆã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚',
                'examples': [g['topic'] for g in not_covered[:5]]
            })

        # ä½ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒˆãƒ”ãƒƒã‚¯
        under_rep = [g for g in self.coverage_gaps if g['status'] == 'under_represented']
        if under_rep:
            recommendations.append({
                'priority': 'medium',
                'category': 'ä½ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒˆãƒ”ãƒƒã‚¯',
                'count': len(under_rep),
                'description': f'{len(under_rep)}å€‹ã®ãƒˆãƒ”ãƒƒã‚¯ãŒã‚½ãƒ¼ã‚¹è³‡æ–™ã§é »å‡ºã™ã‚‹ãŒã€è©¦é¨“å•é¡Œã§ã®å‡ºç¾é »åº¦ãŒä½ã„ã§ã™ã€‚',
                'action': 'ã“ã‚Œã‚‰ã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹å•é¡Œã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚',
                'examples': [g['topic'] for g in under_rep[:5]]
            })

        return recommendations

    def print_summary(self, report):
        """ã‚µãƒãƒªãƒ¼å‡ºåŠ›"""
        print("\n" + "=" * 70)
        print("ğŸ“Š ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—åˆ†æã‚µãƒãƒªãƒ¼")
        print("=" * 70)

        print(f"\nã€ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã€‘")
        print(f"  è©¦é¨“å•é¡Œæ•°: {report['metadata']['total_problems']}å•")
        print(f"  è¬›ç¾©è³‡æ–™ãƒãƒ£ãƒ³ã‚¯æ•°: {report['metadata']['total_lecture_chunks']}")
        print(f"  OCRãƒšãƒ¼ã‚¸æ•°: {report['metadata']['total_ocr_pages']}")

        print(f"\nã€ãƒˆãƒ”ãƒƒã‚¯åˆ†æã€‘")
        print(f"  ã‚½ãƒ¼ã‚¹ãƒˆãƒ”ãƒƒã‚¯æ•°: {report['statistics']['source_topics_count']}")
        print(f"  å•é¡Œãƒˆãƒ”ãƒƒã‚¯æ•°: {report['statistics']['problem_topics_count']}")
        print(f"  æ¤œå‡ºã•ã‚ŒãŸã‚®ãƒ£ãƒƒãƒ—: {report['statistics']['coverage_gaps_count']}")

        print(f"\nã€ãƒˆãƒƒãƒ—10ã‚®ãƒ£ãƒƒãƒ—ã€‘")
        for i, gap in enumerate(report['coverage_gaps'][:10], 1):
            status_icon = "âŒ" if gap['status'] == 'not_covered' else "âš ï¸"
            print(f"  {i}. {status_icon} {gap['topic']}")
            print(f"      ã‚½ãƒ¼ã‚¹: {gap['source_frequency']}å›, å•é¡Œ: {gap['problem_frequency']}å›")

        print(f"\nã€æ¨å¥¨äº‹é …ã€‘")
        for rec in report['recommendations']:
            icon = "ğŸ”´" if rec['priority'] == 'high' else "ğŸŸ¡"
            print(f"\n  {icon} {rec['category']} (å„ªå…ˆåº¦: {rec['priority'].upper()})")
            print(f"     {rec['description']}")
            print(f"     ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {rec['action']}")
            print(f"     ä¾‹: {', '.join(rec['examples'][:3])}")

        print("\n" + "=" * 70)

    def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("=" * 70)
        print("ğŸš€ è©¦é¨“å•é¡Œã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—åˆ†æã‚’é–‹å§‹")
        print("=" * 70)

        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self.load_problems()
            self.load_lecture_materials()
            self.load_ocr_pages()

            # ãƒˆãƒ”ãƒƒã‚¯åˆ†æ
            lecture_topics, _ = self.analyze_lecture_topics()
            problem_topics = self.analyze_problem_topics()

            # ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®š
            self.identify_coverage_gaps(lecture_topics, problem_topics)

            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = self.generate_report()

            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            self.print_summary(report)

            print("\nâœ… åˆ†æå®Œäº†")
            return True

        except Exception as e:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    analyzer = CoverageGapAnalyzer()
    success = analyzer.run()
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
