#!/usr/bin/env python3
"""
RAGãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åŒ– æ¤œç´¢å“è³ªãƒ»å•é¡Œç”Ÿæˆå“è³ªã®æ¤œè¨¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. æ¤œç´¢å“è³ªæ”¹å–„ç¢ºèª: æ—§å®Ÿè£… vs ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ã®æ¯”è¼ƒ
2. å•é¡Œç”Ÿæˆå“è³ªæ¸¬å®š: ç”Ÿæˆã•ã‚Œã‚‹å•é¡Œã®æ­£ç¢ºæ€§ãƒ»æ³•ä»¤æº–æ‹ æ€§è©•ä¾¡
"""

import json
import time
from pathlib import Path
from typing import Dict, List
from datetime import datetime
import logging

from rag_hybrid_search import RAGHybridSearch

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - QUALITY_VAL - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

REPO_ROOT = Path("/home/planj/patshinko-exam-app")
PROBLEMS_FILE = REPO_ROOT / "backend/problems_final_500.json"

# ===== 1. æ¤œç´¢å“è³ªã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ =====

def simple_keyword_search(query: str, legal_texts: Dict[str, str], top_k: int = 3) -> List[Dict]:
    """æ—§å®Ÿè£…: ã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"""
    results = []
    keywords = query.split()

    for ref_name, content in legal_texts.items():
        relevance_score = 0
        for keyword in keywords:
            if keyword in content:
                relevance_score += len([w for w in content.split() if w == keyword])

        if relevance_score > 0:
            results.append({
                'source': ref_name,
                'relevance': relevance_score,
                'method': 'simple_keyword'
            })

    results = sorted(results, key=lambda x: -x['relevance'])[:top_k]
    return results

def hybrid_rag_search(query: str, rag_engine: RAGHybridSearch, top_k: int = 3) -> List[Dict]:
    """æ–°å®Ÿè£…: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGæ¤œç´¢"""
    results = rag_engine.hybrid_search(query, top_k=top_k)
    return [
        {
            'source': r['clause']['title'],
            'hybrid_score': float(r['scores']['hybrid']),
            'bm25_score': float(r['scores']['bm25']),
            'method': 'hybrid_rag'
        }
        for r in results
    ]

def test_search_quality():
    """æ¤œç´¢å“è³ªæ”¹å–„ã®ç¢ºèªãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*80)
    print("ã€ãƒ†ã‚¹ãƒˆ1ã€‘æ¤œç´¢å“è³ªæ”¹å–„ç¢ºèª: æ—§å®Ÿè£… vs ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…")
    print("="*80 + "\n")

    # å•é¡Œèª­ã¿è¾¼ã¿
    with open(PROBLEMS_FILE) as f:
        problems = json.load(f)

    # æ³•ä»¤ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿è¾¼ã¿
    legal_texts = {}
    legal_ref_dir = REPO_ROOT / "rag_data/legal_references"
    for file_path in sorted(legal_ref_dir.glob("*.txt")):
        with open(file_path, encoding='utf-8', errors='ignore') as f:
            legal_texts[file_path.stem] = f.read()

    # RAGãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    print("RAGãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ä¸­...")
    rag_engine = RAGHybridSearch()
    rag_engine.initialize()
    print("âœ… ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†\n")

    # ã‚µãƒ³ãƒ—ãƒ«5å•ã«ã¤ã„ã¦æ¯”è¼ƒ
    sample_problems = problems[:5]
    comparison_results = []

    for i, problem in enumerate(sample_problems, 1):
        problem_text = problem['problem_text'][:80]
        category = problem['category']

        print(f"ã€å•é¡Œ {i}ã€‘{problem_text}...")
        print(f"ã‚«ãƒ†ã‚´ãƒª: {category}\n")

        # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ
        search_query = f"{category} {problem['pattern_name']}"

        # æ—§å®Ÿè£…å®Ÿè¡Œ
        print("  æ—§å®Ÿè£…ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰:")
        old_start = time.time()
        old_results = simple_keyword_search(search_query, legal_texts, top_k=2)
        old_time = time.time() - old_start

        if old_results:
            for j, res in enumerate(old_results, 1):
                print(f"    {j}. {res['source']} (ã‚¹ã‚³ã‚¢: {res['relevance']})")
        else:
            print(f"    â†’ æ¤œç´¢çµæœãªã—")
        print(f"    å®Ÿè¡Œæ™‚é–“: {old_time*1000:.2f}ms\n")

        # æ–°å®Ÿè£…å®Ÿè¡Œ
        print("  æ–°å®Ÿè£…ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGï¼‰:")
        new_start = time.time()
        new_results = hybrid_rag_search(search_query, rag_engine, top_k=2)
        new_time = time.time() - new_start

        if new_results:
            for j, res in enumerate(new_results, 1):
                print(f"    {j}. {res['source']} (ã‚¹ã‚³ã‚¢: {res['hybrid_score']:.3f})")
        else:
            print(f"    â†’ æ¤œç´¢çµæœãªã—")
        print(f"    å®Ÿè¡Œæ™‚é–“: {new_time*1000:.2f}ms\n")

        # æ¯”è¼ƒçµæœè¨˜éŒ²
        comparison_results.append({
            "problem_id": problem['problem_id'],
            "category": category,
            "old_results_count": len(old_results),
            "new_results_count": len(new_results),
            "old_time_ms": old_time * 1000,
            "new_time_ms": new_time * 1000,
            "improvement": "âœ…" if len(new_results) >= len(old_results) else "âš ï¸"
        })

        time.sleep(0.5)

    # çµæœã‚µãƒãƒªãƒ¼
    print("="*80)
    print("ğŸ“Š æ¤œç´¢å“è³ªæ”¹å–„ã‚µãƒãƒªãƒ¼")
    print("="*80)

    old_avg = sum(r['old_results_count'] for r in comparison_results) / len(comparison_results)
    new_avg = sum(r['new_results_count'] for r in comparison_results) / len(comparison_results)

    print(f"\næ—§å®Ÿè£…: å¹³å‡ {old_avg:.1f} ä»¶")
    print(f"æ–°å®Ÿè£…: å¹³å‡ {new_avg:.1f} ä»¶")
    print(f"æ”¹å–„åº¦: {((new_avg - old_avg) / max(old_avg, 1) * 100):.1f}%\n")

    return comparison_results

# ===== 2. å•é¡Œç”Ÿæˆå“è³ªã®æ¸¬å®š =====

def evaluate_problem_quality(problem: Dict, rag_results: Dict) -> Dict:
    """å•é¡Œç”Ÿæˆå“è³ªã®å®šé‡è©•ä¾¡"""
    evaluation = {
        "problem_id": problem['problem_id'],
        "category": problem['category'],
        "scores": {}
    }

    # 1. æ¤œç´¢çµæœã®æœ‰ç„¡
    has_rag_results = rag_results.get('result_count', 0) > 0
    evaluation['scores']['rag_search_found'] = 1.0 if has_rag_results else 0.0

    # 2. ç”Ÿæˆèª¬æ˜ã®é•·ã•ï¼ˆç›®å®‰: 150-250æ–‡å­—ï¼‰
    explanation = problem.get('explanation_hybrid_rag', '')
    explanation_length = len(explanation)

    if 150 <= explanation_length <= 250:
        evaluation['scores']['explanation_length'] = 1.0
    elif 100 <= explanation_length < 150 or 250 < explanation_length <= 300:
        evaluation['scores']['explanation_length'] = 0.7
    elif explanation_length < 100 or explanation_length > 300:
        evaluation['scores']['explanation_length'] = 0.3
    else:
        evaluation['scores']['explanation_length'] = 0.0

    # 3. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¡¨ç¾ã®æ’é™¤
    template_phrases = ['ã«é–¢ã™ã‚‹å•é¡Œã§ã™', 'ã«ã¤ã„ã¦å­¦ã¶', 'ã®çŸ¥è­˜']
    template_found = any(phrase in explanation for phrase in template_phrases)
    evaluation['scores']['no_template'] = 0.0 if template_found else 1.0

    # 4. æ³•ä»¤å‚ç…§ã®æœ‰ç„¡
    has_legal_ref = 'æ¡' in explanation or 'æ³•' in explanation
    evaluation['scores']['legal_reference'] = 1.0 if has_legal_ref else 0.5

    # 5. ç·åˆã‚¹ã‚³ã‚¢ï¼ˆåŠ é‡å¹³å‡ï¼‰
    weights = {
        'rag_search_found': 0.2,
        'explanation_length': 0.25,
        'no_template': 0.35,
        'legal_reference': 0.2
    }

    total_score = sum(
        evaluation['scores'].get(key, 0) * weight
        for key, weight in weights.items()
    )
    evaluation['overall_score'] = round(total_score, 2)

    return evaluation

def test_problem_quality():
    """å•é¡Œç”Ÿæˆå“è³ªã®æ¸¬å®šãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*80)
    print("ã€ãƒ†ã‚¹ãƒˆ2ã€‘å•é¡Œç”Ÿæˆå“è³ªæ¸¬å®š")
    print("="*80 + "\n")

    # ã‚µãƒ³ãƒ—ãƒ«å•é¡Œã§ãƒ†ã‚¹ãƒˆï¼ˆ5å•ï¼‰
    with open(PROBLEMS_FILE) as f:
        problems = json.load(f)

    sample_problems = problems[:5]
    quality_results = []

    print("ã‚µãƒ³ãƒ—ãƒ«5å•ã®å“è³ªã‚’è©•ä¾¡ä¸­...\n")

    for i, problem in enumerate(sample_problems, 1):
        # ãƒ€ãƒŸãƒ¼ã®RAGçµæœï¼ˆå®Ÿéš›ã®çµ±åˆå®Ÿè¡Œã§ã¯å•é¡Œç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰å–å¾—ï¼‰
        rag_results = {
            'result_count': 1 if i != 3 else 0,  # 3ç•ªç›®ã¯æ¤œç´¢ãªã—ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            'results': []
        }

        # ãƒ€ãƒŸãƒ¼èª¬æ˜ã‚’è¨­å®šï¼ˆå®Ÿéš›ã®å®Ÿè¡Œã§ã¯ç”Ÿæˆã•ã‚Œã‚‹ï¼‰
        problem['explanation_hybrid_rag'] = problem.get('explanation', '')[:200]

        quality = evaluate_problem_quality(problem, rag_results)
        quality_results.append(quality)

        print(f"ã€å•é¡Œ {i}ã€‘{problem['problem_text'][:50]}...")
        print(f"  RAGæ¤œç´¢: {'âœ…' if rag_results['result_count'] > 0 else 'âŒ'}")
        print(f"  èª¬æ˜é•·: {len(problem['explanation_hybrid_rag'])} æ–‡å­—")
        print(f"  ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ’é™¤: {'âœ…' if quality['scores']['no_template'] == 1.0 else 'âŒ'}")
        print(f"  æ³•ä»¤å‚ç…§: {'âœ…' if quality['scores']['legal_reference'] == 1.0 else 'âš ï¸'}")
        print(f"  ç·åˆã‚¹ã‚³ã‚¢: {quality['overall_score']:.2f}/1.0\n")

    # å“è³ªã‚µãƒãƒªãƒ¼
    print("="*80)
    print("ğŸ“Š å•é¡Œç”Ÿæˆå“è³ªã‚µãƒãƒªãƒ¼")
    print("="*80)

    avg_score = sum(r['overall_score'] for r in quality_results) / len(quality_results)
    rag_found_rate = sum(1 for r in quality_results if r['scores']['rag_search_found'] == 1.0) / len(quality_results)
    no_template_rate = sum(1 for r in quality_results if r['scores']['no_template'] == 1.0) / len(quality_results)

    print(f"\nç·åˆå“è³ªã‚¹ã‚³ã‚¢: {avg_score:.2f}/1.0")
    print(f"RAGæ¤œç´¢æˆåŠŸç‡: {rag_found_rate*100:.1f}%")
    print(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ’é™¤ç‡: {no_template_rate*100:.1f}%")

    if avg_score >= 0.8:
        print("âœ… å“è³ªåˆ¤å®š: è‰¯å¥½ï¼ˆæœ¬ç•ªæŠ•å…¥å¯èƒ½ï¼‰")
    elif avg_score >= 0.6:
        print("âš ï¸  å“è³ªåˆ¤å®š: è¦æ”¹å–„")
    else:
        print("âŒ å“è³ªåˆ¤å®š: ä¸ååˆ†")

    return quality_results

def main():
    print("\n" + "="*80)
    print("ğŸ§ª RAGãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åŒ– å“è³ªæ¤œè¨¼ãƒ†ã‚¹ãƒˆ")
    print("="*80)

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    search_results = test_search_quality()
    quality_results = test_problem_quality()

    # çµæœä¿å­˜
    output_file = REPO_ROOT / "backend/quality_validation_results.json"
    validation_report = {
        "test_time": datetime.now().isoformat(),
        "search_quality": search_results,
        "problem_quality": quality_results,
        "summary": {
            "total_tests": len(search_results) + len(quality_results),
            "search_improvement_avg": sum(r['new_results_count'] - r['old_results_count'] for r in search_results) / len(search_results),
            "quality_score_avg": sum(r['overall_score'] for r in quality_results) / len(quality_results)
        }
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(validation_report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_file}\n")

if __name__ == "__main__":
    main()
