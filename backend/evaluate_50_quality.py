#!/usr/bin/env python3
"""
50å•ã®å“è³ªè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Phase 3.3: å“è³ªé–¾å€¤é”æˆç¢ºèª
"""

import json
from pathlib import Path
from datetime import datetime

REPO_ROOT = Path("/home/planj/patshinko-exam-app")
PROBLEMS_FILE = REPO_ROOT / "backend/problems_50_hybrid_rag.json"

def evaluate_problem_quality(problem: dict) -> dict:
    """å•é¡Œç”Ÿæˆå“è³ªã®å®šé‡è©•ä¾¡"""
    evaluation = {
        "problem_id": problem.get('problem_id', 'N/A'),
        "category": problem.get('category', 'N/A'),
        "scores": {}
    }

    # 1. RAGæ¤œç´¢çµæœã®æœ‰ç„¡
    has_rag_results = problem.get('rag_search_results', {}).get('result_count', 0) > 0
    evaluation['scores']['rag_search_found'] = 1.0 if has_rag_results else 0.0

    # 2. ç”Ÿæˆèª¬æ˜ã®é•·ã•ï¼ˆç›®å®‰: 150-250æ–‡å­—ï¼‰
    explanation = problem.get('explanation_hybrid_rag', '')
    explanation_length = len(explanation)

    if 150 <= explanation_length <= 250:
        evaluation['scores']['explanation_length'] = 1.0
    elif 100 <= explanation_length < 150 or 250 < explanation_length <= 300:
        evaluation['scores']['explanation_length'] = 0.7
    elif explanation_length < 100 or explanation_length > 300:
        evaluation['scores']['explanation_length'] = 0.3
    else:
        evaluation['scores']['explanation_length'] = 0.0

    # 3. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¡¨ç¾ã®æ’é™¤
    template_phrases = ['ã«é–¢ã™ã‚‹å•é¡Œã§ã™', 'ã«ã¤ã„ã¦å­¦ã¶', 'ã®çŸ¥è­˜']
    template_found = any(phrase in explanation for phrase in template_phrases)
    evaluation['scores']['no_template'] = 0.0 if template_found else 1.0

    # 4. æ³•ä»¤å‚ç…§ã®æœ‰ç„¡
    has_legal_ref = 'æ¡' in explanation or 'æ³•' in explanation
    evaluation['scores']['legal_reference'] = 1.0 if has_legal_ref else 0.5

    # 5. ç·åˆã‚¹ã‚³ã‚¢ï¼ˆåŠ é‡å¹³å‡ï¼‰
    weights = {
        'rag_search_found': 0.2,
        'explanation_length': 0.25,
        'no_template': 0.35,
        'legal_reference': 0.2
    }

    total_score = sum(
        evaluation['scores'].get(key, 0) * weight
        for key, weight in weights.items()
    )
    evaluation['overall_score'] = round(total_score, 2)

    return evaluation

def main():
    print("\n" + "="*80)
    print("ğŸ“Š Phase 3.3: 50å•ã®å“è³ªè©•ä¾¡")
    print("="*80 + "\n")

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open(PROBLEMS_FILE) as f:
        problems = json.load(f)

    print(f"ğŸ“– {len(problems)}å•é¡Œã‚’è©•ä¾¡ä¸­...\n")

    quality_results = []
    for problem in problems:
        quality = evaluate_problem_quality(problem)
        quality_results.append(quality)

    # ã‚µãƒãƒªãƒ¼è¨ˆç®—
    overall_scores = [r['overall_score'] for r in quality_results]
    avg_score = sum(overall_scores) / len(overall_scores) if overall_scores else 0

    rag_found_rate = sum(1 for r in quality_results if r['scores']['rag_search_found'] == 1.0) / len(quality_results) if quality_results else 0
    no_template_rate = sum(1 for r in quality_results if r['scores']['no_template'] == 1.0) / len(quality_results) if quality_results else 0
    legal_ref_rate = sum(1 for r in quality_results if r['scores']['legal_reference'] >= 1.0) / len(quality_results) if quality_results else 0

    # èª¬æ˜é•·ã®çµ±è¨ˆ
    explanation_lengths = [len(p.get('explanation_hybrid_rag', '')) for p in problems]
    avg_length = sum(explanation_lengths) / len(explanation_lengths) if explanation_lengths else 0
    min_length = min(explanation_lengths) if explanation_lengths else 0
    max_length = max(explanation_lengths) if explanation_lengths else 0

    # çµæœè¡¨ç¤º
    print("="*80)
    print("ğŸ“Š å“è³ªè©•ä¾¡ã‚µãƒãƒªãƒ¼")
    print("="*80)

    print(f"\nã€ç·åˆå“è³ªã‚¹ã‚³ã‚¢ã€‘")
    print(f"  å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.2f}/1.0")
    print(f"  ç›®æ¨™å€¤: 0.80ä»¥ä¸Š")
    if avg_score >= 0.80:
        print(f"  åˆ¤å®š: âœ… ç›®æ¨™é”æˆ")
    else:
        print(f"  åˆ¤å®š: âš ï¸ è¦æ”¹å–„ (ä¸è¶³: {0.80 - avg_score:.2f})")

    print(f"\nã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘")
    print(f"  RAGæ¤œç´¢æˆåŠŸç‡: {rag_found_rate*100:.1f}% ({int(rag_found_rate*len(quality_results))}/{len(quality_results)})")
    print(f"  ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ’é™¤ç‡: {no_template_rate*100:.1f}% ({int(no_template_rate*len(quality_results))}/{len(quality_results)})")
    print(f"  æ³•ä»¤å‚ç…§ç‡: {legal_ref_rate*100:.1f}% ({int(legal_ref_rate*len(quality_results))}/{len(quality_results)})")

    print(f"\nã€èª¬æ˜é•·ï¼ˆæ–‡å­—æ•°ï¼‰ã€‘")
    print(f"  å¹³å‡: {avg_length:.0f}æ–‡å­—")
    print(f"  æœ€å°: {min_length}æ–‡å­—")
    print(f"  æœ€å¤§: {max_length}æ–‡å­—")
    print(f"  ç›®æ¨™: 150-250æ–‡å­—")

    # è©³ç´°çµæœä¿å­˜
    report = {
        "timestamp": datetime.now().isoformat(),
        "problem_count": len(problems),
        "summary": {
            "average_score": avg_score,
            "target_score": 0.80,
            "achievement": "âœ… é”æˆ" if avg_score >= 0.80 else "âš ï¸ è¦æ”¹å–„",
            "rag_search_rate": round(rag_found_rate, 3),
            "template_exclusion_rate": round(no_template_rate, 3),
            "legal_reference_rate": round(legal_ref_rate, 3),
            "explanation_length": {
                "average": round(avg_length, 1),
                "min": min_length,
                "max": max_length
            }
        },
        "detailed_results": quality_results
    }

    output_file = REPO_ROOT / "backend/quality_eval_50_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ è©³ç´°çµæœ: {output_file}")

    print("\n" + "="*80)
    if avg_score >= 0.80:
        print("âœ… å“è³ªåŸºæº–é”æˆï¼æœ¬ç•ªæŠ•å…¥æº–å‚™å®Œäº†")
    else:
        print("âš ï¸ å“è³ªæ”¹å–„ãŒå¿…è¦ãªçŠ¶æ…‹")
    print("="*80 + "\n")

    return avg_score

if __name__ == "__main__":
    main()
