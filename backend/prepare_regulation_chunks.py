#!/usr/bin/env python3
"""
Task 5.3: å–¶æ¥­è¦åˆ¶åˆ†é‡ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å–¶æ¥­è¨±å¯ãƒ»å–¶æ¥­åœæ­¢ãƒ»å–¶æ¥­ç¦æ­¢é–¢é€£ãƒ†ãƒ¼ãƒã‹ã‚‰ã€
å•é¡Œç”Ÿæˆç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ã™ã‚‹
"""

import json
import os
from pathlib import Path
from collections import defaultdict

print("=" * 80)
print("ã€Task 5.3: å–¶æ¥­è¦åˆ¶åˆ†é‡ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€‘")
print("=" * 80)

# 1. è¬›ç¿’ãƒ†ãƒ¼ãƒã®ç¢ºèª
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—1: è¬›ç¿’ãƒ†ãƒ¼ãƒã®ç¢ºèª")

lecture_dir = Path("rag_data/lecture_text")
if lecture_dir.exists():
    lecture_files = sorted(lecture_dir.glob("*.txt"))
    print(f"  è¦‹ã¤ã‘ãŸãƒ†ãƒ¼ãƒ: {len(lecture_files)}å€‹")
else:
    print(f"âŒ {lecture_dir} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    lecture_files = []

# 2. å–¶æ¥­è¦åˆ¶åˆ†é‡ãƒ†ãƒ¼ãƒã®åˆ†é¡
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—2: å–¶æ¥­è¦åˆ¶åˆ†é‡ãƒ†ãƒ¼ãƒã‚’åˆ†é¡")

# å–¶æ¥­è¦åˆ¶åˆ†é‡ã«è©²å½“ã™ã‚‹ãƒ†ãƒ¼ãƒã‚’å®šç¾©ï¼ˆå–¶æ¥­è¨±å¯ãƒ»å–¶æ¥­åœæ­¢ãƒ»å–¶æ¥­ç¦æ­¢é–¢é€£ï¼‰
regulation_themes = {
    "business_suspension": [
        "theme_013_å–¶æ¥­åœæ­¢å‘½ä»¤.txt",
        "theme_014_å–¶æ¥­åœæ­¢å‘½ä»¤ã®å†…å®¹.txt",
        "theme_015_å–¶æ¥­åœæ­¢æœŸé–“ã®è¨ˆç®—.txt",
    ],
    "business_prohibition": [
        "theme_016_å–¶æ¥­ç¦æ­¢æ™‚é–“.txt",
    ],
    "business_approval": [
        "theme_017_å–¶æ¥­è¨±å¯ã¨å–¶æ¥­å®Ÿç¸¾ã®é–¢ä¿‚.txt",
        "theme_018_å–¶æ¥­è¨±å¯ã¨å‹å¼æ¤œå®šã®é•ã„.txt",
        "theme_019_å–¶æ¥­è¨±å¯ã®å–æ¶ˆã—è¦ä»¶.txt",
        "theme_020_å–¶æ¥­è¨±å¯ã®å¤±åŠ¹äº‹ç”±.txt",
        "theme_021_å–¶æ¥­è¨±å¯ã®è¡Œæ”¿æ‰‹ç¶šã.txt",
        "theme_022_å–¶æ¥­è¨±å¯ã¯ç„¡æœŸé™æœ‰åŠ¹.txt",
    ]
}

reg_theme_count = sum(len(v) for v in regulation_themes.values())
print(f"""
  å–¶æ¥­è¦åˆ¶åˆ†é‡ãƒ†ãƒ¼ãƒ: {reg_theme_count}å€‹
  ã‚«ãƒ†ã‚´ãƒª:
    - å–¶æ¥­è¨±å¯: {len(regulation_themes['business_approval'])}å€‹
    - å–¶æ¥­åœæ­¢: {len(regulation_themes['business_suspension'])}å€‹
    - å–¶æ¥­ç¦æ­¢: {len(regulation_themes['business_prohibition'])}å€‹
""")

# 3. ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ã®å®šç¾©
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—3: ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥ã‚’å®šç¾©")

chunking_strategy = {
    "method": "logical_units",
    "target_tokens": 500,
    "delimiter": ["ã€‚\n", "ã€‚", "\n\n"],
    "min_chunk_size": 50,
    "max_chunk_size": 1000,
    "categories": list(regulation_themes.keys())
}

print(f"""
  ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æ–¹å¼: {chunking_strategy['method']}
  ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {chunking_strategy['target_tokens']}
  æœ€å°/æœ€å¤§: {chunking_strategy['min_chunk_size']}/{chunking_strategy['max_chunk_size']}
""")

# 4. ãƒ†ãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—4: å–¶æ¥­è¦åˆ¶åˆ†é‡ãƒ†ãƒ¼ãƒãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€")

regulation_chunks = []
category_stats = defaultdict(lambda: {"count": 0, "tokens": 0})

for category, theme_files in regulation_themes.items():
    print(f"\n  ã€{category}ã€‘")

    for theme_file in theme_files:
        theme_path = lecture_dir / theme_file

        if not theme_path.exists():
            print(f"    âš ï¸  {theme_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            continue

        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        try:
            with open(theme_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()

            if not content:
                print(f"    âš ï¸  {theme_file} ãŒç©ºã§ã™")
                continue

            # ç°¡æ˜“çš„ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¨å®š
            token_count = len(content) // 3

            # ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
            chunk = {
                "chunk_id": f"regulation_{category}_{len(regulation_chunks):03d}",
                "category": category,
                "source_file": theme_file,
                "content": content,
                "token_count": token_count,
                "source": "lecture_materials"
            }

            regulation_chunks.append(chunk)
            category_stats[category]["count"] += 1
            category_stats[category]["tokens"] += token_count

            print(f"    âœ“ {theme_file:45} ({token_count:5}ãƒˆãƒ¼ã‚¯ãƒ³)")

        except Exception as e:
            print(f"    âœ— ã‚¨ãƒ©ãƒ¼: {theme_file} - {e}")

# 5. çµ±è¨ˆæƒ…å ±
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—5: çµ±è¨ˆé›†è¨ˆ")

total_chunks = len(regulation_chunks)
total_tokens = sum(s["tokens"] for s in category_stats.values())

print(f"""
  ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {total_chunks}å€‹
  ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_tokens}ãƒˆãƒ¼ã‚¯ãƒ³

  ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆã€‘
""")

for category, stats in category_stats.items():
    tokens = stats["tokens"]
    percentage = (tokens / total_tokens * 100) if total_tokens > 0 else 0
    print(f"    {category:30} {stats['count']:2}å€‹ ({tokens:5}ãƒˆãƒ¼ã‚¯ãƒ³, {percentage:5.1f}%)")

# 6. å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—6: å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©")

output_schema = {
    "metadata": {
        "task": "Task 5.3 - å–¶æ¥­è¦åˆ¶åˆ†é‡ãƒ‡ãƒ¼ã‚¿æº–å‚™",
        "domain": "regulation",
        "total_chunks": total_chunks,
        "total_tokens": total_tokens,
        "chunking_method": "logical_units",
        "source": "lecture_materials"
    },
    "chunking_strategy": chunking_strategy,
    "category_distribution": dict(category_stats),
    "sample_chunks": regulation_chunks[:3]
}

print(f"""
  å‡ºåŠ›å½¢å¼: JSONL (1è¡Œ1ãƒãƒ£ãƒ³ã‚¯) + ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSON

  å„ãƒãƒ£ãƒ³ã‚¯ã®æ§‹é€ :
  {{
    "chunk_id": "unique_id",
    "category": "category_name",
    "source_file": "theme_XXX_...txt",
    "content": "...",
    "token_count": 500,
    "source": "lecture_materials"
  }}
""")

# 7. å–¶æ¥­è¦åˆ¶åˆ†é‡ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—7: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")

output_dir = Path("data")
output_dir.mkdir(exist_ok=True)

# JSONLå½¢å¼ã§ä¿å­˜
jsonl_path = output_dir / "regulation_domain_chunks_prepared.jsonl"
with open(jsonl_path, 'w', encoding='utf-8') as f:
    for chunk in regulation_chunks:
        f.write(json.dumps(chunk, ensure_ascii=False) + "\n")

print(f"  âœ“ JSONLä¿å­˜: {jsonl_path} ({total_chunks}è¡Œ)")

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã‚’ä¿å­˜
metadata_path = output_dir / "regulation_domain_chunks_metadata.json"
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(output_schema, f, indent=2, ensure_ascii=False)

print(f"  âœ“ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {metadata_path}")

# 8. ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
print("\nâœ… ã‚¹ãƒ†ãƒƒãƒ—8: ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒ³ã‚¯è¡¨ç¤º")

if regulation_chunks:
    for i, chunk in enumerate(regulation_chunks[:2], 1):
        print(f"\n  ã€ã‚µãƒ³ãƒ—ãƒ« {i}: {chunk['chunk_id']}ã€‘")
        print(f"    ã‚«ãƒ†ã‚´ãƒª: {chunk['category']}")
        print(f"    ã‚½ãƒ¼ã‚¹: {chunk['source_file']}")
        print(f"    ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {chunk['token_count']}")
        preview = chunk['content'][:100].replace('\n', ' ')
        print(f"    å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {preview}...")

# 9. å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
print("\n" + "=" * 80)
print("ã€Task 5.3 å®Œäº† - å–¶æ¥­è¦åˆ¶åˆ†é‡ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†ã€‘")
print("=" * 80)

print(f"""
âœ… æº–å‚™å®Œäº†ï¼š
  - ãƒãƒ£ãƒ³ã‚¯æ•°: {total_chunks}å€‹
  - ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_tokens}ãƒˆãƒ¼ã‚¯ãƒ³
  - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:
    - {jsonl_path}
    - {metadata_path}

ğŸ“Š å–¶æ¥­è¦åˆ¶åˆ†é‡ã®å†…å®¹ï¼š
  - å–¶æ¥­è¨±å¯ï¼ˆç”³è«‹æ‰‹ç¶šãã€æ›´æ–°ã€è¦ä»¶ï¼‰
  - å–¶æ¥­åœæ­¢ï¼ˆå‘½ä»¤åŸºæº–ã€æ‰‹ç¶šãã€ç¾©å‹™ï¼‰
  - å–¶æ¥­ç¦æ­¢ï¼ˆäº‹ç”±ã€æœŸé–“ã€ä¾‹å¤–ã€è§£é™¤æ‰‹ç¶šãï¼‰

ğŸš€ æ¬¡ã‚¿ã‚¹ã‚¯ï¼ˆTask 5.4ï¼‰ï¼š
  - Claude APIã‚’ä½¿ç”¨ã—ã¦150å•ç”Ÿæˆ
  - è¤‡åˆèªå¯¾å¿œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
  - æŠ€è¡“ç®¡ç†+ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£+å–¶æ¥­è¦åˆ¶ 3åˆ†é‡
  - `output/week5_domain_50_raw.json` ã«å‡ºåŠ›
""")

print("=" * 80)
