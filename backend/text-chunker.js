/**
 * TextChunker - OCRãƒ†ã‚­ã‚¹ãƒˆã¨æ³•å¾‹æ–‡æ›¸ã®ãƒãƒ£ãƒ³ã‚¯åŒ–å‡¦ç†
 *
 * å½¹å‰²:
 * - JSONã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
 * - ã‚»ã‚¯ã‚·ãƒ§ãƒ³å˜ä½ã§ã®ã‚¹ãƒãƒ¼ãƒˆãƒãƒ£ãƒ³ã‚¯åŒ–
 * - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿æŒ
 */

import fs from 'fs';
import path from 'path';

class TextChunker {
  constructor(config = {}) {
    this.chunkSize = config.chunkSize || 800; // æ–‡å­—æ•°
    this.overlapSize = config.overlapSize || 100; // ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
    this.minChunkSize = config.minChunkSize || 100;
  }

  /**
   * OCRçµæœJSONã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãƒãƒ£ãƒ³ã‚¯åŒ–
   * @param {string} jsonPath - ocr_results_corrected.jsonã®ãƒ‘ã‚¹
   * @returns {Array} ãƒãƒ£ãƒ³ã‚¯é…åˆ— [{id, text, page, section, metadata}]
   */
  async chunkOCRResults(jsonPath) {
    console.log(`ğŸ”„ Reading OCR results from: ${jsonPath}`);

    try {
      const rawData = fs.readFileSync(jsonPath, 'utf-8');
      const ocrData = JSON.parse(rawData);

      const chunks = [];
      let globalChunkId = 0;

      // ãƒšãƒ¼ã‚¸ã”ã¨ã«å‡¦ç†
      if (Array.isArray(ocrData)) {
        for (const page of ocrData) {
          const pageNum = page.page_number || page.page || 0;
          const pageContent = page.content || page.text || '';

          // ãƒšãƒ¼ã‚¸å†…ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã•ã‚‰ã«åˆ†å‰²
          const sections = this.splitIntoSections(pageContent, pageNum);

          for (const section of sections) {
            // ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–
            const sectionChunks = this.chunkText(
              section.text,
              `page_${pageNum}_${section.index}`
            );

            for (const chunk of sectionChunks) {
              chunks.push({
                id: `ocr_${globalChunkId++}`,
                text: chunk,
                page: pageNum,
                section: section.title || `Page ${pageNum} Section ${section.index}`,
                source: 'ocr_corrected',
                timestamp: new Date().toISOString(),
                metadata: {
                  fileSize: chunk.length,
                  wordCount: chunk.split(/\s+/).length,
                }
              });
            }
          }
        }
      }

      console.log(`âœ… OCR chunking complete: ${chunks.length} chunks created`);
      return chunks;
    } catch (error) {
      console.error('âŒ Error chunking OCR results:', error);
      throw error;
    }
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³å˜ä½ã§åˆ†å‰²
   * @param {string} text
   * @param {number} pageNum
   * @returns {Array}
   */
  splitIntoSections(text, pageNum) {
    // ç« ãƒ»ç¯€ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ (ä¾‹: "ç¬¬1ç« " "1.1" "1)" ãªã©)
    const sectionPatterns = [
      /^ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+[ç« ç¯€æ¡]/m,
      /^\d+\.\s/m,
      /^\d+\)\s/m,
      /^ã€/m,
    ];

    let sections = [];
    let currentSection = '';
    let sectionIndex = 0;

    const lines = text.split('\n');
    for (const line of lines) {
      const isNewSection = sectionPatterns.some(pattern => pattern.test(line));

      if (isNewSection && currentSection.trim().length > 0) {
        sections.push({
          title: lines[0] || `Page ${pageNum}`,
          text: currentSection.trim(),
          index: sectionIndex++
        });
        currentSection = line + '\n';
      } else {
        currentSection += line + '\n';
      }
    }

    // æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    if (currentSection.trim().length > 0) {
      sections.push({
        title: `Section ${sectionIndex}`,
        text: currentSection.trim(),
        index: sectionIndex
      });
    }

    return sections.length > 0 ? sections : [{
      title: `Page ${pageNum}`,
      text: text,
      index: 0
    }];
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ä»˜ãï¼‰
   * @param {string} text
   * @param {string} sectionId
   * @returns {Array} ãƒãƒ£ãƒ³ã‚¯é…åˆ—
   */
  chunkText(text, sectionId) {
    const chunks = [];

    if (text.length <= this.chunkSize) {
      return [text];
    }

    let start = 0;
    while (start < text.length) {
      let end = Math.min(start + this.chunkSize, text.length);

      // æ–‡å­—æ•°åˆ¶é™å†…ã§æœ€å¾Œã®å¥èª­ç‚¹ã§åˆ‡ã‚‹ï¼ˆè‡ªç„¶ãªåˆ†å‰²ç‚¹ã‚’æ¢ã™ï¼‰
      if (end < text.length) {
        const lastPunctuation = Math.max(
          text.lastIndexOf('ã€‚', end),
          text.lastIndexOf('ã€', end)
        );
        if (lastPunctuation > start + this.minChunkSize) {
          end = lastPunctuation + 1;
        }
      }

      chunks.push(text.substring(start, end));

      // ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®ã—ã¦æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã¸
      start = end - this.overlapSize;
    }

    return chunks;
  }

  /**
   * Markdownãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–
   * @param {string} mdPath - Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
   * @returns {Array}
   */
  async chunkMarkdownFile(mdPath) {
    console.log(`ğŸ”„ Reading Markdown file: ${mdPath}`);

    try {
      const content = fs.readFileSync(mdPath, 'utf-8');
      return this.chunkMarkdown(content, path.basename(mdPath));
    } catch (error) {
      console.error('âŒ Error reading Markdown file:', error);
      throw error;
    }
  }

  /**
   * Markdownã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯åŒ–
   * @param {string} content - Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„
   * @param {string} filename
   * @returns {Array}
   */
  chunkMarkdown(content, filename = 'unknown') {
    const chunks = [];
    let globalChunkId = 0;

    // # ã§å§‹ã¾ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§åˆ†å‰²
    const sections = content.split(/^#+\s+/m).slice(1);

    for (let i = 0; i < sections.length; i++) {
      const section = sections[i];
      const lines = section.split('\n');
      const title = lines[0] || `Section ${i}`;

      // ã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’é™¤ã„ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„
      const sectionContent = lines.slice(1).join('\n').trim();

      if (sectionContent.length === 0) continue;

      // ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç´°ã‹ããƒãƒ£ãƒ³ã‚¯åŒ–
      const textChunks = this.chunkText(sectionContent, title);

      for (const chunk of textChunks) {
        chunks.push({
          id: `md_${globalChunkId++}`,
          text: chunk,
          section: title,
          source: 'markdown',
          sourceFile: filename,
          timestamp: new Date().toISOString(),
          metadata: {
            fileSize: chunk.length,
            wordCount: chunk.split(/\s+/).length,
          }
        });
      }
    }

    console.log(`âœ… Markdown chunking complete: ${chunks.length} chunks created from ${filename}`);
    return chunks;
  }

  /**
   * è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸€æ‹¬ãƒãƒ£ãƒ³ã‚¯åŒ–
   * @param {Object} sources - {ocr: path, markdown: [paths]}
   * @returns {Object} {ocrChunks, mdChunks}
   */
  async chunkMultipleSources(sources) {
    const results = {
      ocrChunks: [],
      mdChunks: [],
      totalChunks: 0
    };

    if (sources.ocr) {
      try {
        results.ocrChunks = await this.chunkOCRResults(sources.ocr);
      } catch (error) {
        console.error('Error processing OCR source:', error);
      }
    }

    if (sources.markdown && Array.isArray(sources.markdown)) {
      for (const mdPath of sources.markdown) {
        try {
          const mdChunks = await this.chunkMarkdownFile(mdPath);
          results.mdChunks.push(...mdChunks);
        } catch (error) {
          console.error(`Error processing Markdown ${mdPath}:`, error);
        }
      }
    }

    results.totalChunks = results.ocrChunks.length + results.mdChunks.length;
    console.log(`ğŸ“Š Total chunks created: ${results.totalChunks}`);

    return results;
  }

  /**
   * ãƒãƒ£ãƒ³ã‚¯ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
   * @param {Array} chunks
   * @param {string} outputPath
   */
  saveChunks(chunks, outputPath) {
    try {
      fs.writeFileSync(
        outputPath,
        JSON.stringify(chunks, null, 2),
        'utf-8'
      );
      console.log(`âœ… Chunks saved to: ${outputPath}`);
    } catch (error) {
      console.error('âŒ Error saving chunks:', error);
      throw error;
    }
  }
}

export { TextChunker };
