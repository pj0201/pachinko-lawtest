#!/usr/bin/env python3
"""
æ­£ã—ã„246å•ã®è©¦é¨“å•é¡Œã‚’ä½¿ç”¨ã—ãŸã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—åˆ†æ

6ã¤ã®ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«:
1. è¬›ç¾©è³‡æ–™â‘ .pdf
2. è¬›ç¾©è³‡æ–™â‘¡.pdf
3. è¬›ç¾©è³‡æ–™â‘¢.pdf
4. é¢¨å–¶æ³•.pdf
5. é¢¨å–¶æ³•æ–½è¡Œè¦å‰‡.pdf
6. éŠæŠ€æ©Ÿå–æ‰±ä¸»ä»»è€…ã«é–¢ã™ã‚‹è¦å®š + å®Ÿæ–½è¦é ˜
"""

import json
import re
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime
import sys

class DetailedCoverageAnalyzer:
    """è©³ç´°ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æï¼ˆ246å•ç‰ˆï¼‰"""

    def __init__(self):
        self.base_dir = Path(__file__).parent.parent
        self.problems = []
        self.ocr_pages = []

        # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®ãƒˆãƒ”ãƒƒã‚¯
        self.source_topics = {
            'lecture_1': defaultdict(list),  # â‘ .pdf
            'lecture_2': defaultdict(list),  # â‘¡.pdf
            'lecture_3': defaultdict(list),  # â‘¢.pdf
            'fueiho': defaultdict(list),     # é¢¨å–¶æ³•
            'fueiho_rules': defaultdict(list),  # é¢¨å–¶æ³•æ–½è¡Œè¦å‰‡
            'supervisor_rules': defaultdict(list)  # ä¸»ä»»è€…è¦å®š
        }

        self.problem_covered_topics = set()
        self.uncovered_items = []

    def load_problems(self):
        """æ­£ã—ã„246å•ã‚’èª­ã¿è¾¼ã‚€"""
        print("ğŸ“– è©¦é¨“å•é¡Œï¼ˆ246å•ï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")

        problems_file = self.base_dir / "backend" / "db" / "problems.json"
        with open(problems_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.problems = data.get('problems', [])

        print(f"   âœ… å•é¡Œæ•°: {len(self.problems)}å•")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        categories = Counter([p.get('category', 'unknown') for p in self.problems])
        print(f"   ã‚«ãƒ†ã‚´ãƒªåˆ¥:")
        for cat, count in categories.most_common():
            print(f"      - {cat}: {count}å•")

        return len(self.problems)

    def load_lecture_materials(self):
        """è¬›ç¾©è³‡æ–™ï¼ˆâ‘ â‘¡â‘¢ï¼‰ã‚’èª­ã¿è¾¼ã‚€"""
        print("\nğŸ“š è¬›ç¾©è³‡æ–™ï¼ˆâ‘ â‘¡â‘¢.pdfï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")

        ocr_file = self.base_dir / "data" / "old_problems" / "ocr_results_corrected.json"
        with open(ocr_file, 'r', encoding='utf-8') as f:
            self.ocr_pages = json.load(f)

        # PDFåˆ¥ã«åˆ†é¡
        pdf_counts = Counter([p.get('pdf_index') for p in self.ocr_pages])
        print(f"   âœ… ç·ãƒšãƒ¼ã‚¸æ•°: {len(self.ocr_pages)}")
        for pdf_idx in sorted(pdf_counts.keys()):
            print(f"      - PDF{pdf_idx}: {pdf_counts[pdf_idx]}ãƒšãƒ¼ã‚¸")

        return len(self.ocr_pages)

    def extract_structured_topics(self, text, source_type):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ§‹é€ åŒ–ã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º"""
        topics = []

        # 1. ç« ãƒ»ç¯€ã®æŠ½å‡º
        chapter_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+ç« \s+([^\n]+)',
            r'ç¬¬\d+ç« \s+([^\n]+)',
        ]
        for pattern in chapter_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                topics.append({
                    'type': 'chapter',
                    'content': match.strip(),
                    'source': source_type
                })

        # 2. æ¡æ–‡ã®æŠ½å‡º
        article_patterns = [
            r'ç¬¬(\d+)æ¡(?:ã®(\d+))?\s+([^\n]{10,100})',
            r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+)æ¡\s+([^\n]{10,100})',
        ]
        for pattern in article_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                if len(match) == 3:
                    article_num = match[0]
                    sub_num = match[1] if match[1] else ""
                    content = match[2]
                    topics.append({
                        'type': 'article',
                        'article_number': f"ç¬¬{article_num}æ¡{sub_num}",
                        'content': content.strip()[:100],
                        'source': source_type
                    })

        # 3. é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»æ¦‚å¿µ
        key_concepts = [
            # åˆ¶åº¦é–¢é€£
            (r'éŠæŠ€æ©Ÿå–æ‰±ä¸»ä»»è€…(?:ã®|ã«é–¢ã™ã‚‹)([^\nã€‚]{10,80})', 'supervisor_system'),
            (r'è²©å£²æ¥­è€…ç™»éŒ²åˆ¶åº¦([^\nã€‚]{10,80})', 'seller_registration'),
            (r'è£½é€ æ¥­è€…([^\nã€‚]{10,80})', 'manufacturer'),
            (r'èªå®š(?:ç”³è«‹|æ‰‹ç¶š|è¨¼)([^\nã€‚]{10,80})', 'certification'),
            (r'å‹å¼æ¤œå®š([^\nã€‚]{10,80})', 'type_inspection'),

            # æ‰‹ç¶šãé–¢é€£
            (r'ä¿è¨¼æ›¸([^\nã€‚]{10,80})', 'warranty_document'),
            (r'æ¤œæŸ»([^\nã€‚]{10,80})', 'inspection'),
            (r'å±Šå‡º([^\nã€‚]{10,80})', 'notification'),
            (r'ç”³è«‹(?:æ›¸|æ‰‹ç¶š)([^\nã€‚]{10,80})', 'application'),

            # æŠ€è¡“åŸºæº–
            (r'å°„å¹¸å¿ƒ([^\nã€‚]{10,80})', 'gambling_nature'),
            (r'æ€§èƒ½åŸºæº–([^\nã€‚]{10,80})', 'performance_standard'),
            (r'æŠ€è¡“åŸºæº–([^\nã€‚]{10,80})', 'technical_standard'),
            (r'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£([^\nã€‚]{10,80})', 'security'),
            (r'åŸºæ¿(?:ã®|ã«é–¢ã™ã‚‹)([^\nã€‚]{10,80})', 'circuit_board'),
            (r'ãƒãƒƒãƒ—(?:ã®|ã«é–¢ã™ã‚‹)([^\nã€‚]{10,80})', 'chip'),

            # æ¥­å‹™ãƒ»å®Ÿå‹™
            (r'ä¿å®ˆç®¡ç†([^\nã€‚]{10,80})', 'maintenance'),
            (r'ä¸æ­£(?:æ”¹é€ |ä½¿ç”¨|è¡Œç‚º)([^\nã€‚]{10,80})', 'fraud'),
            (r'ä¸­å¤éŠæŠ€æ©Ÿ([^\nã€‚]{10,80})', 'used_machine'),
            (r'ãƒªã‚µã‚¤ã‚¯ãƒ«([^\nã€‚]{10,80})', 'recycling'),

            # ç½°å‰‡ãƒ»ç¾©å‹™
            (r'ç½°å‰‡([^\nã€‚]{10,80})', 'penalty'),
            (r'å–¶æ¥­åœæ­¢([^\nã€‚]{10,80})', 'business_suspension'),
            (r'å–æ¶ˆã—([^\nã€‚]{10,80})', 'cancellation'),
            (r'éµå®ˆäº‹é …([^\nã€‚]{10,80})', 'compliance'),
        ]

        for pattern, concept_type in key_concepts:
            matches = re.findall(pattern, text)
            for match in matches:
                topics.append({
                    'type': 'concept',
                    'concept_type': concept_type,
                    'content': match.strip(),
                    'source': source_type
                })

        return topics

    def analyze_source_content(self):
        """å„ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’åˆ†æ"""
        print("\nğŸ” ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’åˆ†æä¸­...")

        # è¬›ç¾©è³‡æ–™ã®åˆ†æ
        for page in self.ocr_pages:
            pdf_idx = page.get('pdf_index')
            text = page.get('text', '')

            source_key = f'lecture_{pdf_idx}'
            topics = self.extract_structured_topics(text, source_key)

            for topic in topics:
                key = f"{topic['type']}:{topic.get('article_number', topic.get('concept_type', 'other'))}"
                self.source_topics[source_key][key].append({
                    'page': page.get('page_number'),
                    'content': topic.get('content', '')[:100]
                })

        # çµ±è¨ˆè¡¨ç¤º
        print(f"\n   ã‚½ãƒ¼ã‚¹åˆ¥ãƒˆãƒ”ãƒƒã‚¯æ•°:")
        for source, topics in self.source_topics.items():
            if topics:
                print(f"      - {source}: {len(topics)}ãƒˆãƒ”ãƒƒã‚¯")

        return self.source_topics

    def analyze_problem_coverage(self):
        """å•é¡ŒãŒã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†æ"""
        print("\nğŸ” å•é¡Œã®ã‚«ãƒãƒ¼å†…å®¹ã‚’åˆ†æä¸­...")

        for problem in self.problems:
            question = problem.get('question', '')
            options = ' '.join([opt.get('text', '') for opt in problem.get('options', [])])
            explanation = problem.get('explanation', '')

            full_text = f"{question} {options} {explanation}"

            # ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡º
            topics = self.extract_structured_topics(full_text, 'problem')
            for topic in topics:
                key = f"{topic['type']}:{topic.get('article_number', topic.get('concept_type', 'other'))}"
                self.problem_covered_topics.add(key)

        print(f"   âœ… å•é¡ŒãŒã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯: {len(self.problem_covered_topics)}")
        return self.problem_covered_topics

    def identify_uncovered_content(self):
        """æœªã‚«ãƒãƒ¼ã®å†…å®¹ã‚’ç‰¹å®š"""
        print("\nğŸ” æœªã‚«ãƒãƒ¼å†…å®¹ã‚’ç‰¹å®šä¸­...")

        uncovered = []

        for source_name, topics in self.source_topics.items():
            for topic_key, occurrences in topics.items():
                if topic_key not in self.problem_covered_topics:
                    # å‡ºç¾é »åº¦ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ2å›ä»¥ä¸Šå‡ºç¾ï¼‰
                    if len(occurrences) >= 2:
                        uncovered.append({
                            'source': source_name,
                            'topic_key': topic_key,
                            'frequency': len(occurrences),
                            'pages': [occ['page'] for occ in occurrences[:5]],
                            'sample_content': occurrences[0]['content'] if occurrences else ''
                        })

        # é »åº¦ã§ã‚½ãƒ¼ãƒˆ
        uncovered.sort(key=lambda x: x['frequency'], reverse=True)
        self.uncovered_items = uncovered

        print(f"   âœ… æœªã‚«ãƒãƒ¼é …ç›®: {len(uncovered)}")
        return uncovered

    def generate_detailed_report(self):
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\nğŸ“Š è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

        # ã‚½ãƒ¼ã‚¹åˆ¥ã«æœªã‚«ãƒãƒ¼é …ç›®ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        by_source = defaultdict(list)
        for item in self.uncovered_items:
            by_source[item['source']].append(item)

        report_lines = [
            "# éŠæŠ€æ©Ÿå–æ‰±ä¸»ä»»è€…è©¦é¨“ ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—è©³ç´°åˆ†æ",
            "",
            f"**åˆ†ææ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            f"**è©¦é¨“å•é¡Œæ•°**: {len(self.problems)}å•",
            f"**ã‚½ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸æ•°**: {len(self.ocr_pages)}ãƒšãƒ¼ã‚¸",
            "",
            "---",
            "",
            "## ğŸ“Š åˆ†æã‚µãƒãƒªãƒ¼",
            "",
            f"- **å•é¡ŒãŒã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯**: {len(self.problem_covered_topics)}",
            f"- **æœªã‚«ãƒãƒ¼ã®é …ç›®**: {len(self.uncovered_items)}",
            "",
        ]

        # ã‚½ãƒ¼ã‚¹åˆ¥ã®è©³ç´°
        for source_name in ['lecture_1', 'lecture_2', 'lecture_3']:
            items = by_source.get(source_name, [])
            if not items:
                continue

            pdf_num = source_name.split('_')[1]
            report_lines.extend([
                "",
                f"## ğŸ“„ è¬›ç¾©è³‡æ–™{['â‘ ', 'â‘¡', 'â‘¢'][int(pdf_num)-1]}ã®æœªã‚«ãƒãƒ¼å†…å®¹",
                "",
                f"**æœªã‚«ãƒãƒ¼é …ç›®æ•°**: {len(items)}",
                ""
            ])

            # ãƒˆãƒ”ãƒƒã‚¯ã‚¿ã‚¤ãƒ—åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            by_type = defaultdict(list)
            for item in items:
                topic_type = item['topic_key'].split(':')[0]
                by_type[topic_type].append(item)

            # æ¡æ–‡
            if by_type['article']:
                report_lines.extend([
                    "### ğŸ“œ æœªã‚«ãƒãƒ¼ã®æ¡æ–‡",
                    "",
                    "| æ¡æ–‡ | å‡ºç¾å›æ•° | ãƒšãƒ¼ã‚¸ | å†…å®¹ã‚µãƒ³ãƒ—ãƒ« |",
                    "|------|---------|--------|------------|"
                ])
                for item in sorted(by_type['article'], key=lambda x: x['frequency'], reverse=True)[:20]:
                    article = item['topic_key'].split(':')[1]
                    pages = ', '.join([f"p{p}" for p in item['pages'][:3]])
                    content = item['sample_content'][:50]
                    report_lines.append(f"| {article} | {item['frequency']}å› | {pages} | {content}... |")
                report_lines.append("")

            # æ¦‚å¿µ
            if by_type['concept']:
                report_lines.extend([
                    "### ğŸ’¡ æœªã‚«ãƒãƒ¼ã®é‡è¦æ¦‚å¿µ",
                    "",
                    "| æ¦‚å¿µ | å‡ºç¾å›æ•° | ãƒšãƒ¼ã‚¸ | å†…å®¹ã‚µãƒ³ãƒ—ãƒ« |",
                    "|------|---------|--------|------------|"
                ])
                for item in sorted(by_type['concept'], key=lambda x: x['frequency'], reverse=True)[:20]:
                    concept = item['topic_key'].split(':')[1]
                    pages = ', '.join([f"p{p}" for p in item['pages'][:3]])
                    content = item['sample_content'][:50]
                    report_lines.append(f"| {concept} | {item['frequency']}å› | {pages} | {content}... |")
                report_lines.append("")

            # ç« 
            if by_type['chapter']:
                report_lines.extend([
                    "### ğŸ“– æœªã‚«ãƒãƒ¼ã®ç« ãƒ»ç¯€",
                    "",
                    "| ç« ãƒ»ç¯€ | å‡ºç¾å›æ•° | ãƒšãƒ¼ã‚¸ |",
                    "|--------|---------|--------|"
                ])
                for item in sorted(by_type['chapter'], key=lambda x: x['frequency'], reverse=True)[:10]:
                    chapter = item['topic_key'].split(':')[1]
                    pages = ', '.join([f"p{p}" for p in item['pages'][:3]])
                    report_lines.append(f"| {chapter} | {item['frequency']}å› | {pages} |")
                report_lines.append("")

        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        report_lines.extend([
            "",
            "## ğŸ¯ æ¨å¥¨ã•ã‚Œã‚‹å•é¡Œä½œæˆ",
            "",
            "### å„ªå…ˆåº¦ï¼šHIGHï¼ˆå‡ºç¾5å›ä»¥ä¸Šï¼‰",
            ""
        ])

        high_priority = [item for item in self.uncovered_items if item['frequency'] >= 5]
        for i, item in enumerate(high_priority[:20], 1):
            source_label = item['source'].replace('lecture_', 'è¬›ç¾©è³‡æ–™')
            if 'lecture' in item['source']:
                pdf_num = item['source'].split('_')[1]
                source_label = f"è¬›ç¾©è³‡æ–™{['â‘ ', 'â‘¡', 'â‘¢'][int(pdf_num)-1]}"

            topic_parts = item['topic_key'].split(':')
            topic_name = topic_parts[1] if len(topic_parts) > 1 else topic_parts[0]

            report_lines.append(f"{i}. **{topic_name}** ({source_label})")
            report_lines.append(f"   - å‡ºç¾å›æ•°: {item['frequency']}å›")
            report_lines.append(f"   - ãƒšãƒ¼ã‚¸: {', '.join([f'p{p}' for p in item['pages'][:5]])}")
            report_lines.append(f"   - å†…å®¹: {item['sample_content']}")
            report_lines.append("")

        report_content = '\n'.join(report_lines)

        # ä¿å­˜
        output_file = self.base_dir / "backend" / "data" / "detailed_coverage_gap_246.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        print(f"   âœ… è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_file}")

        return report_content

    def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("=" * 70)
        print("ğŸš€ æ­£ã—ã„246å•ã«ã‚ˆã‚‹è©³ç´°ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ")
        print("=" * 70)

        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self.load_problems()
            self.load_lecture_materials()

            # åˆ†æå®Ÿè¡Œ
            self.analyze_source_content()
            self.analyze_problem_coverage()
            self.identify_uncovered_content()

            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = self.generate_detailed_report()

            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            print("\n" + "=" * 70)
            print("ğŸ“Š åˆ†æå®Œäº†ã‚µãƒãƒªãƒ¼")
            print("=" * 70)
            print(f"âœ… å•é¡Œæ•°: {len(self.problems)}å•")
            print(f"âœ… ã‚«ãƒãƒ¼æ¸ˆã¿ãƒˆãƒ”ãƒƒã‚¯: {len(self.problem_covered_topics)}")
            print(f"âŒ æœªã‚«ãƒãƒ¼é …ç›®: {len(self.uncovered_items)}")
            print(f"\næœ€ã‚‚é‡è¦ãªæœªã‚«ãƒãƒ¼é …ç›®ï¼ˆãƒˆãƒƒãƒ—10ï¼‰:")
            for i, item in enumerate(self.uncovered_items[:10], 1):
                topic_name = item['topic_key'].split(':')[1] if ':' in item['topic_key'] else item['topic_key']
                print(f"  {i}. {topic_name}: {item['frequency']}å›")
            print("=" * 70)

            return True

        except Exception as e:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    analyzer = DetailedCoverageAnalyzer()
    success = analyzer.run()
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
