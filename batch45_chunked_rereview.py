#!/usr/bin/env python3
"""
Batch 4-5 å†ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¿®å¾©å‡¦ç†ï¼ˆãƒãƒ£ãƒ³ã‚¯åŒ–å¯¾å¿œï¼‰
140å•ã¨78å•ã‚’70+70, 40+38ã«åˆ†å‰²ã—ã¦å‡¦ç†
"""

import json
import os
from openai import OpenAI
import time

api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

def process_batch_rereview_chunked(batch_name, total_problems, chunk_size):
    """ãƒãƒƒãƒã‚’ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã—ã¦å†ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Ÿæ–½"""
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ {batch_name} ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å†ãƒ¬ãƒ“ãƒ¥ãƒ¼é–‹å§‹")
    print(f"{'='*70}")
    print(f"   ç·å•é¡Œæ•°: {total_problems}å•")
    print(f"   ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}å•")
    print(f"   é–‹å§‹: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("")
    
    # Load correction results
    correction_file = f"data/{batch_name.upper()}_CORRECTION_RESULTS.txt"
    with open(correction_file, 'r', encoding='utf-8') as f:
        correction_content = f.read()
    
    # Load original data
    if batch_name == "batch4":
        with open('data/BATCH_4_REVIEW_DATA.json', 'r', encoding='utf-8') as f:
            batch_data = json.load(f)
        problems = batch_data['problems']
    else:  # batch5
        with open('data/BATCH_5_REVIEW_DATA.json', 'r', encoding='utf-8') as f:
            batch_data = json.load(f)
        problems = batch_data['problems']
    
    all_results = []
    
    # Process chunks
    num_chunks = (total_problems + chunk_size - 1) // chunk_size
    
    for chunk_idx in range(num_chunks):
        start_idx = chunk_idx * chunk_size
        end_idx = min((chunk_idx + 1) * chunk_size, total_problems)
        chunk_problems = problems[start_idx:end_idx]
        chunk_num = chunk_idx + 1
        
        print(f"â³ Chunk {chunk_num}/{num_chunks}: å•é¡Œ {chunk_problems[0]['problem_id']}-{chunk_problems[-1]['problem_id']} ({len(chunk_problems)}å•)")
        
        # Create prompt for this chunk
        problems_str = "\n".join([
            f"{p['problem_id']}: [{p['theme_name']}] {p['problem_text'][:70]}... ç­”:{p['correct_answer']}"
            for p in chunk_problems
        ])
        
        # Use only small portion of correction content to stay within limits
        correction_preview = correction_content[:1500]
        
        prompt = f"""ã€å†è©•ä¾¡å¯¾è±¡ã€‘ä¸»ä»»è€…è¬›ç¿’è©¦é¨“ãƒ»æ³•å¾‹å•é¡Œ {len(chunk_problems)}å•ï¼ˆ{batch_name.upper()} ä¿®æ­£å¾Œï¼‰
ãƒãƒ£ãƒ³ã‚¯ {chunk_num}/{num_chunks}

ã€ä¿®æ­£å†…å®¹ã®ä¸€éƒ¨ã€‘
{correction_preview}

ã€å†è©•ä¾¡åŸºæº–ã€‘ï¼ˆä¿®æ­£å¾Œã®å“è³ªæœ€çµ‚ç¢ºèªï¼‰
- æ³•çš„æ ¹æ‹ ã®å…·ä½“æ€§: æ¡æ–‡ç•ªå·ãŒæ˜è¨˜ã•ã‚Œã¦ã„ã‚‹ã‹ï¼ˆ10ç‚¹ï¼‰
- å•é¡Œæ–‡ã¨è§£èª¬ã®ä¸€è‡´: å®Œå…¨ã«å¯¾å¿œã—ã¦ã„ã‚‹ã‹ï¼ˆ10ç‚¹ï¼‰
- æŠ½è±¡è¡¨ç¾ã®æœ‰ç„¡: ã€Œä¸€å®šã®ã€ã€Œé©åˆ‡ãªã€ãªã©æ›–æ˜§ãªè¡¨ç¾ãŒãªã„ã‹ï¼ˆ10ç‚¹ï¼‰

ã€æ¡ç‚¹åŸºæº–ã€‘
- 24ç‚¹ä»¥ä¸Š: âœ…åˆæ ¼
- 19ï½23ç‚¹: âš ï¸è¦æ”¹å–„
- 18ç‚¹ä»¥ä¸‹: âŒä¸åˆæ ¼

ã€å‡ºåŠ›å½¢å¼ã€‘
å„å•é¡Œã‚’1è¡Œã§ï¼šID: ã‚¹ã‚³ã‚¢ç‚¹ | âœ…/âš ï¸/âŒ | ç†ç”±

ä¿®æ­£å¾Œã®å…¨{len(chunk_problems)}å•ã‚’ä¸Šè¨˜åŸºæº–ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ï¼š

{problems_str}"""
        
        try:
            response = client.chat.completions.create(
                model="gpt-5-mini",
                messages=[
                    {"role": "system", "content": "ä¸»ä»»è€…è¬›ç¿’è©¦é¨“å•é¡Œã®å³å¯†ãªè©•ä¾¡è€…ã€‚ä¿®æ­£å¾Œã®å•é¡Œã‚’æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_completion_tokens=16000
            )
            
            result = response.choices[0].message.content
            all_results.append(result)
            
            # Count results for this chunk
            pass_count = result.count('âœ…')
            improve_count = result.count('âš ï¸')
            fail_count = result.count('âŒ')
            
            print(f"   âœ… {pass_count}å• | âš ï¸ {improve_count}å• | âŒ {fail_count}å•")
            print(f"   ãƒˆãƒ¼ã‚¯ãƒ³: {response.usage.prompt_tokens + response.usage.completion_tokens}ãƒˆãƒ¼ã‚¯ãƒ³")
            
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        # Small delay between chunks
        if chunk_idx < num_chunks - 1:
            time.sleep(2)
    
    # Merge all results
    merged_result = "\n".join(all_results)
    
    # Save merged results
    output_file = f"data/{batch_name.upper()}_REREVIEW_RESULTS.txt"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"ã€{batch_name.upper()} Stage 3 - å†ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœï¼ˆãƒãƒ£ãƒ³ã‚¯å‡¦ç†ç‰ˆï¼‰ã€‘\n")
        f.write(f"å®Ÿæ–½: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"å¯¾è±¡: {total_problems}å•ï¼ˆä¿®æ­£å¾Œã®æœ€çµ‚è©•ä¾¡ï¼‰\n")
        f.write(f"ãƒ¢ãƒ‡ãƒ«: gpt-5-miniï¼ˆãƒãƒ£ãƒ³ã‚¯å‡¦ç†: {num_chunks}åˆ†å‰²ï¼‰\n")
        f.write("=" * 70 + "\n\n")
        f.write(merged_result)
    
    # Final statistics
    total_pass = merged_result.count('âœ…')
    total_improve = merged_result.count('âš ï¸')
    total_fail = merged_result.count('âŒ')
    
    print(f"\nâœ… {batch_name.upper()} ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†")
    print(f"   ä¿å­˜å…ˆ: {output_file}")
    print(f"   ã€æœ€çµ‚çµ±è¨ˆã€‘")
    print(f"   âœ… åˆæ ¼: {total_pass}å•")
    print(f"   âš ï¸  è¦æ”¹å–„: {total_improve}å•")
    print(f"   âŒ ä¸åˆæ ¼: {total_fail}å•")
    print(f"   è¨ˆ: {total_pass + total_improve + total_fail}å•")
    
    return True

# Main execution
if __name__ == "__main__":
    print("ğŸ”§ Batch 4-5 ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å†ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Ÿè¡Œ")
    print("")
    
    # Batch 4: 140å•ã‚’70+70ã«åˆ†å‰²
    success_b4 = process_batch_rereview_chunked("batch4", 140, 70)
    
    # Batch 5: 78å•ã‚’40+38ã«åˆ†å‰²
    success_b5 = process_batch_rereview_chunked("batch5", 78, 40)
    
    if success_b4 and success_b5:
        print("\n" + "="*70)
        print("ğŸ‰ å…¨ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†ï¼")
        print("="*70)
        print("ã“ã‚Œã§Backendå®Ÿè£…ãŒè‡ªå‹•é–‹å§‹ã§ãã¾ã™")
    else:
        print("\nâŒ å‡¦ç†ã«å¤±æ•—ã—ãŸé …ç›®ãŒã‚ã‚Šã¾ã™")

